{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec0f982-68d8-4fc4-8c40-f3841a9c9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, udf, regexp_replace, lit, from_unixtime\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType, StringType, MapType\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, MapType, StringType\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import split, explode, regexp_extract, col, collect_list, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, FloatType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a502793-bc5f-4e96-a753-22bfb658195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop existing Spark context if any\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Reinitialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BehaviorsProcessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "111b3241-5686-4261-95c0-751debc6b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"  # Replace with your Java path\n",
    "os.environ[\"SPARK_HOME\"] = \"/path/to/spark\"  # Replace with your Spark installation path\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"  # Replace with your Python path\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9448e8fd-8103-4d5e-b2d8-8cdc939e959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ImpressionID: string (nullable = true)\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- History: string (nullable = true)\n",
      " |-- Impressions: string (nullable = true)\n",
      "\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "|ImpressionID|UserID|Time                 |History                                                       |Impressions      |\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "|1           |U13740|11/11/2019 9:05:58 AM|N55189 N42782 N34694 N45794 N18445 N63302 N10414 N19347 N31801|N55689-1 N35729-0|\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "behaviors_schema = StructType([\n",
    "    StructField(\"ImpressionID\", StringType(), True),\n",
    "    StructField(\"UserID\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"History\", StringType(), True),\n",
    "    StructField(\"Impressions\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the behaviors.tsv file\n",
    "behaviors_df = spark.read.csv(\n",
    "    \"data/mind/MINDsmall_train/behaviors.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    schema=behaviors_schema,\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Display the schema and a sample row\n",
    "behaviors_df.printSchema()\n",
    "behaviors_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3137302-b6af-48b3-922a-e71c88a1cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------------------------------------------------------------------+\n",
      "|ImpressionID|UserID|HistoryList                                                             |\n",
      "+------------+------+------------------------------------------------------------------------+\n",
      "|1           |U13740|[N55189, N42782, N34694, N45794, N18445, N63302, N10414, N19347, N31801]|\n",
      "+------------+------+------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split History into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"HistoryList\", split(col(\"History\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"History\")  # Drop original History column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"UserID\", \"HistoryList\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50637a47-9a55-47c5-9898-deedc22f558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|ImpressionID|ImpressionsList     |\n",
      "+------------+--------------------+\n",
      "|1           |[N55689-1, N35729-0]|\n",
      "+------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split Impressions into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"ImpressionsList\", split(col(\"Impressions\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"Impressions\")  # Drop original Impressions column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"ImpressionsList\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f03662b-9d9e-4055-875f-af051a5f7d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------+----------+\n",
      "|ImpressionID|UserID|CandidateNewsID|ClickLabel|\n",
      "+------------+------+---------------+----------+\n",
      "|1           |U13740|N55689         |1         |\n",
      "|1           |U13740|N35729         |0         |\n",
      "|2           |U91836|N20678         |0         |\n",
      "|2           |U91836|N39317         |0         |\n",
      "|2           |U91836|N58114         |0         |\n",
      "+------------+------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode ImpressionsList\n",
    "impressions_exploded = behaviors_df.select(\n",
    "    \"ImpressionID\",\n",
    "    \"UserID\",\n",
    "    \"Time\",\n",
    "    \"HistoryList\",\n",
    "    explode(\"ImpressionsList\").alias(\"ImpressionItem\")\n",
    ")\n",
    "\n",
    "# Extract CandidateNewsID and ClickLabel using regex\n",
    "impressions_exploded = impressions_exploded \\\n",
    "    .withColumn(\"CandidateNewsID\", regexp_extract(col(\"ImpressionItem\"), r\"^(N\\d+)-\\d+$\", 1)) \\\n",
    "    .withColumn(\"ClickLabel\", regexp_extract(col(\"ImpressionItem\"), r\"^N\\d+-(\\d+)$\", 1).cast(\"integer\")) \\\n",
    "    .drop(\"ImpressionItem\")\n",
    "\n",
    "# Verify the transformation\n",
    "impressions_exploded.select(\"ImpressionID\", \"UserID\", \"CandidateNewsID\", \"ClickLabel\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eef02ba-8117-4d3b-a442-d86f7df1406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tfidf_path = \"news_tfidf.parquet\"\n",
    "news_features_df = spark.read.parquet(news_tfidf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244ebec2-1d16-4446-b3f5-e3e65762be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|NewsID|       TFIDFFeatures|\n",
      "+------+--------------------+\n",
      "| N5727|(10000,[1,9,87,90...|\n",
      "|N25908|(10000,[165,280,4...|\n",
      "| N2490|(10000,[1,218,262...|\n",
      "|  N192|(10000,[71,2006,2...|\n",
      "| N1298|(10000,[3,26,42,4...|\n",
      "|N57313|(10000,[36,61,109...|\n",
      "|N36185|(10000,[6,21,74,8...|\n",
      "|N33743|(10000,[2,13,16,3...|\n",
      "|N58255|(10000,[0,10,14,3...|\n",
      "|N44291|(10000,[0,4,15,84...|\n",
      "|N38233|(10000,[5,7,11,69...|\n",
      "| N1970|(10000,[4,26,30,3...|\n",
      "|N41692|(10000,[4,12,31,1...|\n",
      "|N31209|(10000,[4,34,59,1...|\n",
      "|N60452|(10000,[154,159,2...|\n",
      "|N22043|(10000,[36,119,25...|\n",
      "|N30368|(10000,[45,815,12...|\n",
      "| N4233|(10000,[11,90,121...|\n",
      "|N51387|(10000,[111,865,1...|\n",
      "|N22126|(10000,[4,18,34,3...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "809a45c8-3fdf-4859-ae0f-f2a258b1f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join impressions with news_features_df on CandidateNewsID\n",
    "impressions_with_features = impressions_exploded.join(\n",
    "    news_features_df,\n",
    "    impressions_exploded.CandidateNewsID == news_features_df.NewsID,\n",
    "    how=\"left\"\n",
    ").drop(news_features_df.NewsID)  # Drop duplicate NewsID column if present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "238ed1d5-faf6-48b4-8973-1bcc35dfa673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------+----------+--------------------+\n",
      "|ImpressionID|UserID|CandidateNewsID|ClickLabel|       TFIDFFeatures|\n",
      "+------------+------+---------------+----------+--------------------+\n",
      "|           1|U13740|         N55689|         1|(10000,[8,30,51,9...|\n",
      "+------------+------+---------------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the join\n",
    "impressions_with_features.select(\"ImpressionID\", \"UserID\", \"CandidateNewsID\", \"ClickLabel\", \"TFIDFFeatures\").show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80c87016-1cd3-4dad-b5a5-b2214b1300c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------------------+--------------------+---------------+----------+--------------------+\n",
      "|ImpressionID|UserID|                Time|         HistoryList|CandidateNewsID|ClickLabel|       TFIDFFeatures|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+--------------------+\n",
      "|           1|U13740|11/11/2019 9:05:5...|[N55189, N42782, ...|         N55689|         1|(10000,[8,30,51,9...|\n",
      "|           2|U91836|11/12/2019 6:11:3...|[N31739, N6072, N...|         N17059|         1|(10000,[25,38,108...|\n",
      "|           3|U73700|11/14/2019 7:01:4...|[N10732, N25792, ...|         N23814|         1|(10000,[43,100,14...|\n",
      "|           4|U34670|11/11/2019 5:28:0...|[N45729, N2203, N...|         N49685|         1|(10000,[128,170,2...|\n",
      "|           5| U8125|11/12/2019 4:11:2...|[N10078, N56514, ...|          N8400|         1|(10000,[1,63,79,1...|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter records where ClickLabel == 1\n",
    "clicked_news_df = impressions_with_features.filter(col(\"ClickLabel\") == 1)\n",
    "\n",
    "# Verify the filtered DataFrame\n",
    "clicked_news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eb62de2-2179-4ca1-a57a-bda89fbef94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, collect_list, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "# Define a UDF to average a list of SparseVectors or DenseVectors\n",
    "def average_vectors(sum_vec, count):\n",
    "    if count == 0:  # Avoid division by zero\n",
    "        return Vectors.dense([0.0] * len(sum_vec[0]))  # Handle empty vector gracefully\n",
    "    return Vectors.dense([sum(x) / count for x in zip(*[v.toArray() for v in sum_vec])])\n",
    "\n",
    "avg_vector_udf = udf(average_vectors, VectorUDT())\n",
    "\n",
    "# Filter only clicked news to build user profiles\n",
    "clicked_news_df = impressions_with_features.filter(col(\"ClickLabel\") == 1)\n",
    "\n",
    "# Collect clicked TF-IDF vectors by user\n",
    "user_profiles = clicked_news_df.groupBy(\"UserID\").agg(\n",
    "    collect_list(\"TFIDFFeatures\").alias(\"UserVectors\"),\n",
    "    size(collect_list(\"TFIDFFeatures\")).alias(\"CountVectors\")\n",
    ")\n",
    "# Compute the averaged user profile vector\n",
    "user_profiles = user_profiles.withColumn(\n",
    "    \"UserProfile\",\n",
    "    avg_vector_udf(\"UserVectors\", \"CountVectors\")\n",
    ").drop(\"UserVectors\", \"CountVectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45857efe-92f2-45c3-b29d-1d34f3854918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|UserID|         UserProfile|\n",
      "+------+--------------------+\n",
      "|U10022|[0.53004950030122...|\n",
      "+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "user_profiles.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cdf23a-d8dc-4d4b-8bdc-50e081a5fb0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_profiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m cosine_udf \u001b[38;5;241m=\u001b[39m udf(cosine_similarity_udf, FloatType())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Cross join user_profiles with news_features_df (Be aware: this can be huge for large datasets!)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# For large datasets, you'd want to use more scalable approximate methods.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m user_recs \u001b[38;5;241m=\u001b[39m \u001b[43muser_profiles\u001b[49m\u001b[38;5;241m.\u001b[39mcrossJoin(news_features_df) \\\n\u001b[1;32m     13\u001b[0m                          \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, cosine_udf(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserProfile\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTFIDFFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user_profiles' is not defined"
     ]
    }
   ],
   "source": [
    "def cosine_similarity_udf(u_vec, i_vec):\n",
    "    # Convert Spark vectors to numpy\n",
    "    u_arr = u_vec.toArray()\n",
    "    i_arr = i_vec.toArray()\n",
    "    sim = np.dot(u_arr, i_arr) / (np.linalg.norm(u_arr) * np.linalg.norm(i_arr))\n",
    "    return float(sim)\n",
    "\n",
    "cosine_udf = udf(cosine_similarity_udf, FloatType())\n",
    "\n",
    "# Cross join user_profiles with news_features_df (Be aware: this can be huge for large datasets!)\n",
    "user_recs = user_profiles.crossJoin(broadcast(news_features_df)) \\\n",
    "                         .withColumn(\"similarity\", cosine_udf(col(\"UserProfile\"), col(\"TFIDFFeatures\")))\n",
    "user_recs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a438bb1-d505-47dc-8bcf-81c34949aa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd267b7-cdf7-48a5-a24c-5a5a324f2808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c632756-1431-499d-9f39-2b028cb7e4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18df652-63e2-4de7-bb19-ad343fcd8532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb1f09-1470-438f-bf55-500ad2ae9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_recs.show(2)\n",
    "!pip show py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1084668-dd3e-4aee-89d0-50726edee1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedaf366-9e6c-4cc4-ad76-457ea2ff534f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf17156-9c6f-4e7d-a214-0e3d7510664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Define a window partitioned by UserID and ordered by descending similarity\n",
    "windowSpec = Window.partitionBy(\"UserID\").orderBy(F.desc(\"similarity\"))\n",
    "\n",
    "# Select top-N (for example, top-10) articles per user\n",
    "top_n = user_recs.withColumn(\"rank\", F.row_number().over(windowSpec)) \\\n",
    "                 .filter(F.col(\"rank\") <= 10) \\\n",
    "                 .select(\"UserID\", \"NewsID\", \"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208237ba-6ee1-4b31-81a1-b69c1a95fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
