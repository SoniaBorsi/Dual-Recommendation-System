{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, udf, regexp_replace, lit, from_unixtime\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType, StringType, MapType\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, MapType, StringType\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import split, explode, regexp_extract, col, collect_list, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, FloatType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 16:03:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MIND Dataset Processing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|                  []|\n",
      "|N19639|   health|     weightloss|50 Worst Habits F...|These seemingly h...|https://assets.ms...|[{\"Label\": \"Adipo...|[{\"Label\": \"Adipo...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the news.tsv file\n",
    "news_path = \"data/mind/MINDsmall_train/news.tsv\"\n",
    "\n",
    "# Define column names for the news.tsv file\n",
    "news_columns = [\"NewsID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"]\n",
    "\n",
    "# Load the news.tsv file into a Spark DataFrame\n",
    "news_df = spark.read.csv(\n",
    "    news_path,\n",
    "    sep=\"\\t\",\n",
    "    schema=\"NewsID STRING, Category STRING, Subcategory STRING, Title STRING, Abstract STRING, URL STRING, TitleEntities STRING, AbstractEntities STRING\",\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Assign column names\n",
    "news_df.show(n=2, truncate=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PRE PROCESSING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, length, size, udf\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 16:03:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|                  []|\n",
      "|N19639|   health|     weightloss|50 Worst Habits F...|These seemingly h...|https://assets.ms...|[{\"Label\": \"Adipo...|[{\"Label\": \"Adipo...|\n",
      "|N61837|     news|      newsworld|The Cost of Trump...|Lt. Ivan Molchane...|https://assets.ms...|                  []|[{\"Label\": \"Ukrai...|\n",
      "|N53526|   health|         voices|I Was An NBA Wife...|I felt like I was...|https://assets.ms...|                  []|[{\"Label\": \"Natio...|\n",
      "|N38324|   health|        medical|How to Get Rid of...|They seem harmles...|https://assets.ms...|[{\"Label\": \"Skin ...|[{\"Label\": \"Skin ...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PreprocessingPipeline\").getOrCreate()\n",
    "\n",
    "# Load your data (modify the path as necessary)\n",
    "news_df = spark.read.csv(\"data/mind/MINDsmall_train/news.tsv\", sep=\"\\t\", header=False, inferSchema=True)\n",
    "\n",
    "# Assign column names\n",
    "news_df = news_df.toDF(\"NewsID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\")\n",
    "\n",
    "# Display initial rows\n",
    "news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping missing values: 48616\n"
     ]
    }
   ],
   "source": [
    "### MISSING VALUES ###\n",
    "\n",
    "# Drop rows where Title or Abstract are missing\n",
    "news_df = news_df.na.drop(subset=[\"Title\", \"Abstract\"])\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Rows after dropping missing values: {news_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          CleanTitle|       CleanAbstract|\n",
      "+--------------------+--------------------+\n",
      "|the brands queen ...|shop the notebook...|\n",
      "|50 worst habits f...|these seemingly h...|\n",
      "|the cost of trump...|lt. ivan molchane...|\n",
      "|i was an nba wife...|i felt like i was...|\n",
      "|how to get rid of...|they seem harmles...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=21871) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### TEXT CLEANING ###\n",
    "\n",
    "# Define a function to clean text (remove special characters and convert to lowercase)\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        return text.lower().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "\n",
    "# Apply text cleaning to Title and Abstract\n",
    "news_df = news_df.withColumn(\"CleanTitle\", clean_text_udf(col(\"Title\")))\n",
    "news_df = news_df.withColumn(\"CleanAbstract\", clean_text_udf(col(\"Abstract\")))\n",
    "\n",
    "# Display cleaned text\n",
    "news_df.select(\"CleanTitle\", \"CleanAbstract\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         TitleTokens|      AbstractTokens|\n",
      "+--------------------+--------------------+\n",
      "|[the, brands, que...|[shop, the, noteb...|\n",
      "|[50, worst, habit...|[these, seemingly...|\n",
      "|[the, cost, of, t...|[lt., ivan, molch...|\n",
      "|[i, was, an, nba,...|[i, felt, like, i...|\n",
      "|[how, to, get, ri...|[they, seem, harm...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TOKENIZATION ###\n",
    "\n",
    "# Tokenize CleanTitle and CleanAbstract\n",
    "tokenizer_title = Tokenizer(inputCol=\"CleanTitle\", outputCol=\"TitleTokens\")\n",
    "tokenizer_abstract = Tokenizer(inputCol=\"CleanAbstract\", outputCol=\"AbstractTokens\")\n",
    "\n",
    "news_df = tokenizer_title.transform(news_df)\n",
    "news_df = tokenizer_abstract.transform(news_df)\n",
    "\n",
    "# Display tokenized data\n",
    "news_df.select(\"TitleTokens\", \"AbstractTokens\").show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "| FilteredTitleTokens|FilteredAbstractTokens|\n",
      "+--------------------+----------------------+\n",
      "|[brands, queen, e...|  [shop, notebooks,...|\n",
      "|[50, worst, habit...|  [seemingly, harml...|\n",
      "|[cost, trump's, a...|  [lt., ivan, molch...|\n",
      "|[nba, wife., affe...|  [felt, like, frau...|\n",
      "|[get, rid, skin, ...|  [seem, harmless,,...|\n",
      "+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### STOPWORDS REMOVAL ###\n",
    "\n",
    "# Remove stopwords from TitleTokens and AbstractTokens\n",
    "stopword_remover_title = StopWordsRemover(inputCol=\"TitleTokens\", outputCol=\"FilteredTitleTokens\")\n",
    "stopword_remover_abstract = StopWordsRemover(inputCol=\"AbstractTokens\", outputCol=\"FilteredAbstractTokens\")\n",
    "\n",
    "news_df = stopword_remover_title.transform(news_df)\n",
    "news_df = stopword_remover_abstract.transform(news_df)\n",
    "\n",
    "# Display filtered tokens\n",
    "news_df.select(\"FilteredTitleTokens\", \"FilteredAbstractTokens\").show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to clean each token in the array\n",
    "def clean_tokens(tokens):\n",
    "    if tokens:\n",
    "        return [token.replace(\",\", \"\") for token in tokens]  # Remove commas\n",
    "    return tokens\n",
    "\n",
    "# Register the UDF\n",
    "clean_tokens_udf = udf(clean_tokens, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to FilteredTitleTokens\n",
    "news_df = news_df.withColumn(\"FilteredTitleTokens\", clean_tokens_udf(col(\"FilteredTitleTokens\")))\n",
    "news_df = news_df.withColumn(\"FilteredAbstractTokens\", clean_tokens_udf(col(\"FilteredAbstractTokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               Title| FilteredTitleTokens|\n",
      "+--------------------+--------------------+\n",
      "|The Brands Queen ...|[brands, queen, e...|\n",
      "|50 Worst Habits F...|[50, worst, habit...|\n",
      "|The Cost of Trump...|[cost, trump's, a...|\n",
      "|I Was An NBA Wife...|[nba, wife., affe...|\n",
      "|How to Get Rid of...|[get, rid, skin, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"Title\", \"FilteredTitleTokens\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|            Abstract|FilteredAbstractTokens|\n",
      "+--------------------+----------------------+\n",
      "|Shop the notebook...|  [shop, notebooks,...|\n",
      "|These seemingly h...|  [seemingly, harml...|\n",
      "+--------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"Abstract\", \"FilteredAbstractTokens\").show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities',\n",
       " 'CleanTitle',\n",
       " 'CleanAbstract',\n",
       " 'TitleTokens',\n",
       " 'AbstractTokens',\n",
       " 'FilteredTitleTokens',\n",
       " 'FilteredAbstractTokens']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+----------------------+\n",
      "|NewsID| Category| FilteredTitleTokens|FilteredAbstractTokens|\n",
      "+------+---------+--------------------+----------------------+\n",
      "|N55528|lifestyle|[brands, queen, e...|  [shop, notebooks,...|\n",
      "+------+---------+--------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select('NewsID', 'Category', 'FilteredTitleTokens', 'FilteredAbstractTokens').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp==5.5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (5.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spark-nlp==5.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EMBEDDINGS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.base import DocumentAssembler, TokenAssembler\n",
    "from sparknlp.annotator import BertEmbeddings\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.sql.functions import col, concat_ws, array_union, explode\n",
    "from pyspark.sql.functions import concat, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NewsID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Subcategory: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Abstract: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- TitleEntities: string (nullable = true)\n",
      " |-- AbstractEntities: string (nullable = true)\n",
      " |-- CleanTitle: string (nullable = true)\n",
      " |-- CleanAbstract: string (nullable = true)\n",
      " |-- TitleTokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- AbstractTokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- FilteredTitleTokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- FilteredAbstractTokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.withColumn(\n",
    "    \"combined_tokens\", concat(col(\"FilteredTitleTokens\"), col(\"FilteredAbstractTokens\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     combined_tokens|\n",
      "+--------------------+\n",
      "|[brands, queen, e...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select('combined_tokens').show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Term frequency \n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"combined_tokens\", outputCol=\"raw_features\")\n",
    "cv_model = cv.fit(news_df)\n",
    "news_df_tf = cv_model.transform(news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 16:07:02 WARN DAGScheduler: Broadcasting large task binary with size 1236.8 KiB\n",
      "24/12/12 16:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1237.8 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf\")\n",
    "idf_model = idf.fit(news_df_tf)\n",
    "news_df_tfidf = idf_model.transform(news_df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 16:07:27 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|NewsID|     combined_tokens|              tf_idf|\n",
      "+------+--------------------+--------------------+\n",
      "|N55528|[brands, queen, e...|(109675,[309,978,...|\n",
      "|N19639|[50, worst, habit...|(109675,[27,437,8...|\n",
      "|N61837|[cost, trump's, a...|(109675,[63,176,3...|\n",
      "|N53526|[nba, wife., affe...|(109675,[38,89,23...|\n",
      "|N38324|[get, rid, skin, ...|(109675,[6,17,20,...|\n",
      "| N2073|[nfl, able, fine,...|(109675,[87,94,17...|\n",
      "|N49186|[orlando's, hotte...|(109675,[98,225,2...|\n",
      "|N59295|[chile:, three, d...|(109675,[2,19,23,...|\n",
      "|N24510|[best, ps5, games...|(109675,[13,28,33...|\n",
      "|N39237|[report, weather-...|(109675,[52,65,93...|\n",
      "| N9721|[50, foods, never...|(109675,[17,187,2...|\n",
      "|N60905|[trying, make, ra...|(109675,[1,44,58,...|\n",
      "|N39758|[25, biggest, gro...|(109675,[5,206,22...|\n",
      "|N28361|[instagram, filte...|(109675,[142,181,...|\n",
      "|N18680|[michigan, apple,...|(109675,[156,181,...|\n",
      "|N55610|[kate, middleton'...|(109675,[33,35,41...|\n",
      "|N35621|[stars, got, fire...|(109675,[27,72,87...|\n",
      "|N22850|[newark, liberty,...|(109675,[5,8,711,...|\n",
      "|N58173|[2021, gmc, yukon...|(109675,[100,557,...|\n",
      "|N29120|[john, dorsey, ad...|(109675,[2,7,14,1...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df_tfidf.select(\"NewsID\", \"combined_tokens\", \"tf_idf\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "|ImpressionID|UserID|Time                 |History                                                       |Impressions      |\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "|1           |U13740|11/11/2019 9:05:58 AM|N55189 N42782 N34694 N45794 N18445 N63302 N10414 N19347 N31801|N55689-1 N35729-0|\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "behaviors_schema = StructType([\n",
    "    StructField(\"ImpressionID\", StringType(), True),\n",
    "    StructField(\"UserID\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"History\", StringType(), True),\n",
    "    StructField(\"Impressions\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the behaviors.tsv file\n",
    "behaviors_df = spark.read.csv(\n",
    "    \"data/mind/MINDsmall_train/behaviors.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    schema=behaviors_schema,\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Display the schema and a sample row\n",
    "# behaviors_df.printSchema()\n",
    "behaviors_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------------------------------------------------------------------+\n",
      "|ImpressionID|UserID|HistoryList                                                             |\n",
      "+------------+------+------------------------------------------------------------------------+\n",
      "|1           |U13740|[N55189, N42782, N34694, N45794, N18445, N63302, N10414, N19347, N31801]|\n",
      "+------------+------+------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split History into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"HistoryList\", split(col(\"History\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"History\")  # Drop original History column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"UserID\", \"HistoryList\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|ImpressionID|ImpressionsList     |\n",
      "+------------+--------------------+\n",
      "|1           |[N55689-1, N35729-0]|\n",
      "+------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split Impressions into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"ImpressionsList\", split(col(\"Impressions\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"Impressions\")  # Drop original Impressions column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"ImpressionsList\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------+----------+\n",
      "|ImpressionID|UserID|CandidateNewsID|ClickLabel|\n",
      "+------------+------+---------------+----------+\n",
      "|1           |U13740|N55689         |1         |\n",
      "|1           |U13740|N35729         |0         |\n",
      "|2           |U91836|N20678         |0         |\n",
      "|2           |U91836|N39317         |0         |\n",
      "|2           |U91836|N58114         |0         |\n",
      "+------------+------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode ImpressionsList\n",
    "impressions_exploded = behaviors_df.select(\n",
    "    \"ImpressionID\",\n",
    "    \"UserID\",\n",
    "    \"Time\",\n",
    "    \"HistoryList\",\n",
    "    explode(\"ImpressionsList\").alias(\"ImpressionItem\")\n",
    ")\n",
    "\n",
    "# Extract CandidateNewsID and ClickLabel using regex\n",
    "impressions_exploded = impressions_exploded \\\n",
    "    .withColumn(\"CandidateNewsID\", regexp_extract(col(\"ImpressionItem\"), r\"^(N\\d+)-\\d+$\", 1)) \\\n",
    "    .withColumn(\"ClickLabel\", regexp_extract(col(\"ImpressionItem\"), r\"^N\\d+-(\\d+)$\", 1).cast(\"integer\")) \\\n",
    "    .drop(\"ImpressionItem\")\n",
    "\n",
    "# Verify the transformation\n",
    "impressions_exploded.select(\"ImpressionID\", \"UserID\", \"CandidateNewsID\", \"ClickLabel\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join impressions with news_features_df on CandidateNewsID\n",
    "impressions_with_features = impressions_exploded.join(\n",
    "    news_df_tfidf,\n",
    "    impressions_exploded.CandidateNewsID == news_df_tfidf.NewsID,\n",
    "    how=\"left\"\n",
    ").drop(news_df_tfidf.NewsID)  # Drop duplicate NewsID column if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 16:18:37 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/12/12 16:18:42 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "|ImpressionID|UserID|                Time|         HistoryList|CandidateNewsID|ClickLabel|    Category|       Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|          CleanTitle|       CleanAbstract|         TitleTokens|      AbstractTokens| FilteredTitleTokens|FilteredAbstractTokens|     combined_tokens|        raw_features|              tf_idf|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "|      143656|U74335|11/11/2019 3:07:0...|[N13008, N19593, ...|         N11483|         0|       autos|         autossuvs|2020 Hyundai Venu...|Hyundai's new tin...|https://assets.ms...|                  []|[{\"Label\": \"Nissa...|2020 hyundai venu...|hyundai's new tin...|[2020, hyundai, v...|[hyundai's, new, ...|[2020, hyundai, v...|  [hyundai's, new, ...|[2020, hyundai, v...|(109675,[1,85,301...|(109675,[1,85,301...|\n",
      "|       81980|U11232|11/10/2019 1:26:1...|[N63248, N43558, ...|         N29341|         0|   lifestyle|lifestyleparenting|100 vintage baby ...|Stacker looks at ...|https://assets.ms...|                  []|[{\"Label\": \"Bible...|100 vintage baby ...|stacker looks at ...|[100, vintage, ba...|[stacker, looks, ...|[100, vintage, ba...|  [stacker, looks, ...|[100, vintage, ba...|(109675,[27,213,2...|(109675,[27,213,2...|\n",
      "|       61593|U71196|11/14/2019 10:15:...|[N25300, N11101, ...|         N59841|         0|foodanddrink|           recipes|32 Things You Can...|Looking for a hea...|https://assets.ms...|[{\"Label\": \"Air f...|[{\"Label\": \"Air f...|32 things you can...|looking for a hea...|[32, things, you,...|[looking, for, a,...|[32, things, make...|  [looking, healthi...|[32, things, make...|(109675,[1,6,44,1...|(109675,[1,6,44,1...|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "impressions_with_features.show(3, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 16:18:48 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/12/12 16:18:53 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------------------+--------------------+---------------+----------+------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "|ImpressionID|UserID|                Time|         HistoryList|CandidateNewsID|ClickLabel|    Category|     Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|          CleanTitle|       CleanAbstract|         TitleTokens|      AbstractTokens| FilteredTitleTokens|FilteredAbstractTokens|     combined_tokens|        raw_features|              tf_idf|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "|       20621|U21649|11/9/2019 5:15:06 PM|[N14734, N48044, ...|         N24176|         1|      health|      weightloss|Ree Drummond Says...|The Pioneer Woman...|https://assets.ms...|[{\"Label\": \"Ree D...|                  []|ree drummond says...|the pioneer woman...|[ree, drummond, s...|[the, pioneer, wo...|[ree, drummond, s...|  [pioneer, woman's...|[ree, drummond, s...|(109675,[1,15,126...|(109675,[1,15,126...|\n",
      "|       81980|U11232|11/10/2019 1:26:1...|[N63248, N43558, ...|         N26084|         1|foodanddrink|         recipes|These Christmas D...|We've got you cov...|https://assets.ms...|                  []|                  []|these christmas d...|we've got you cov...|[these, christmas...|[we've, got, you,...|[christmas, dinne...|  [got, covered, ty...|[christmas, dinne...|(109675,[120,559,...|(109675,[120,559,...|\n",
      "|       61593|U71196|11/14/2019 10:15:...|[N25300, N11101, ...|         N32005|         1|      travel| traveltripideas|Some of the World...|You may know and ...|https://assets.ms...|                  []|                  []|some of the world...|you may know and ...|[some, of, the, w...|[you, may, know, ...|[world's, best, b...|  [may, know, love,...|[world's, best, b...|(109675,[28,33,66...|(109675,[28,33,66...|\n",
      "|      102498|U68128|11/13/2019 12:56:...|[N34930, N50, N46...|          N4247|         1|      health|       nutrition|The keto diet is ...|Let's face it: He...|https://assets.ms...|                  []|                  []|the keto diet is ...|let's face it: he...|[the, keto, diet,...|[let's, face, it:...|[keto, diet, hard...|  [face, it:, healt...|[keto, diet, hard...|(109675,[0,137,18...|(109675,[0,137,18...|\n",
      "|      143656|U74335|11/11/2019 3:07:0...|[N13008, N19593, ...|         N50059|         1|       autos|autosenthusiasts|This Ford GT40 Mo...|It's half Ford GT...|https://assets.ms...|[{\"Label\": \"Ford ...|[{\"Label\": \"Ford ...|this ford gt40 mo...|it's half ford gt...|[this, ford, gt40...|[it's, half, ford...|[ford, gt40, movi...|  [half, ford, gt40...|[ford, gt40, movi...|(109675,[79,300,4...|(109675,[79,300,4...|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter records where ClickLabel == 1\n",
    "clicked_news_df = impressions_with_features.filter(col(\"ClickLabel\") == 1)\n",
    "\n",
    "# Verify the filtered DataFrame\n",
    "clicked_news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector, SparseVector\n",
    "\n",
    "# Define a function to compute cosine similarity without numpy\n",
    "def cosine_similarity(u_vec, i_vec):\n",
    "    if u_vec is None or i_vec is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Ensure vectors are of type DenseVector or SparseVector\n",
    "    if not isinstance(u_vec, (DenseVector, SparseVector)) or not isinstance(i_vec, (DenseVector, SparseVector)):\n",
    "        return 0.0\n",
    "    \n",
    "    # Convert vectors to lists\n",
    "    u_arr = u_vec.toArray().tolist()\n",
    "    i_arr = i_vec.toArray().tolist()\n",
    "    \n",
    "    # Compute dot product\n",
    "    dot_product = sum(uv * iv for uv, iv in zip(u_arr, i_arr))\n",
    "    \n",
    "    # Compute norms\n",
    "    norm_u = sum(uv ** 2 for uv in u_arr) ** 0.5\n",
    "    norm_i = sum(iv ** 2 for iv in i_arr) ** 0.5\n",
    "    \n",
    "    if norm_u == 0 or norm_i == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm_u * norm_i)\n",
    "\n",
    "# Register the UDF\n",
    "cosine_udf = udf(cosine_similarity, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "\n",
    "# Define a function to compute the average of vectors without numpy\n",
    "def average_vectors(vectors):\n",
    "    if not vectors:\n",
    "        return Vectors.dense([])\n",
    "    \n",
    "    # Convert all vectors to lists\n",
    "    list_of_vectors = [v.toArray().tolist() for v in vectors if v is not None]\n",
    "    \n",
    "    if not list_of_vectors:\n",
    "        return Vectors.dense([])\n",
    "    \n",
    "    # Sum each component\n",
    "    summed_vector = [sum(components) for components in zip(*list_of_vectors)]\n",
    "    \n",
    "    # Compute average\n",
    "    count = len(list_of_vectors)\n",
    "    averaged_vector = [x / count for x in summed_vector]\n",
    "    \n",
    "    return Vectors.dense(averaged_vector)\n",
    "\n",
    "# Register the UDF\n",
    "avg_vector_udf = udf(average_vectors, VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Create the Normalizer object\n",
    "normalizer = Normalizer(inputCol=\"tf_idf\", outputCol=\"ItemNormalized\", p=2.0)\n",
    "\n",
    "# Apply the normalizer to the DataFrame\n",
    "news_df_normalized = normalizer.transform(news_df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Join impressions with normalized news to get candidate embeddings\n",
    "impressions_with_normalized_features = impressions_exploded.join(\n",
    "    news_df_normalized.select(\"NewsID\", \"ItemNormalized\"),\n",
    "    impressions_exploded.CandidateNewsID == news_df_normalized.NewsID,\n",
    "    how=\"left\"\n",
    ").drop(news_df_normalized.NewsID)\n",
    "\n",
    "# Join with normalized user embeddings\n",
    "user_candidate_df = impressions_with_normalized_features.join(\n",
    "    user_embeddings_normalized.select(\"UserID\", \"UserNormalized\"),\n",
    "    on=\"UserID\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
