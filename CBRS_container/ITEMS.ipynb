{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/09 16:42:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MIND Dataset Processing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|                  []|\n",
      "|N19639|   health|     weightloss|50 Worst Habits F...|These seemingly h...|https://assets.ms...|[{\"Label\": \"Adipo...|[{\"Label\": \"Adipo...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the news.tsv file\n",
    "news_path = \"data/mind/MINDsmall_train/news.tsv\"\n",
    "\n",
    "# Define column names for the news.tsv file\n",
    "news_columns = [\"NewsID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"]\n",
    "\n",
    "# Load the news.tsv file into a Spark DataFrame\n",
    "news_df = spark.read.csv(\n",
    "    news_path,\n",
    "    sep=\"\\t\",\n",
    "    schema=\"NewsID STRING, Category STRING, Subcategory STRING, Title STRING, Abstract STRING, URL STRING, TitleEntities STRING, AbstractEntities STRING\",\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Assign column names\n",
    "news_df.show(n=2, truncate=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PRE PROCESSING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, length, size, udf\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/09 16:42:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|                  []|\n",
      "|N19639|   health|     weightloss|50 Worst Habits F...|These seemingly h...|https://assets.ms...|[{\"Label\": \"Adipo...|[{\"Label\": \"Adipo...|\n",
      "|N61837|     news|      newsworld|The Cost of Trump...|Lt. Ivan Molchane...|https://assets.ms...|                  []|[{\"Label\": \"Ukrai...|\n",
      "|N53526|   health|         voices|I Was An NBA Wife...|I felt like I was...|https://assets.ms...|                  []|[{\"Label\": \"Natio...|\n",
      "|N38324|   health|        medical|How to Get Rid of...|They seem harmles...|https://assets.ms...|[{\"Label\": \"Skin ...|[{\"Label\": \"Skin ...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PreprocessingPipeline\").getOrCreate()\n",
    "\n",
    "# Load your data (modify the path as necessary)\n",
    "news_df = spark.read.csv(\"data/mind/MINDsmall_train/news.tsv\", sep=\"\\t\", header=False, inferSchema=True)\n",
    "\n",
    "# Assign column names\n",
    "news_df = news_df.toDF(\"NewsID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\")\n",
    "\n",
    "# Display initial rows\n",
    "news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping missing values: 48616\n"
     ]
    }
   ],
   "source": [
    "### MISSING VALUES ###\n",
    "\n",
    "# Drop rows where Title or Abstract are missing\n",
    "news_df = news_df.na.drop(subset=[\"Title\", \"Abstract\"])\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Rows after dropping missing values: {news_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          CleanTitle|       CleanAbstract|\n",
      "+--------------------+--------------------+\n",
      "|the brands queen ...|shop the notebook...|\n",
      "|50 worst habits f...|these seemingly h...|\n",
      "|the cost of trump...|lt. ivan molchane...|\n",
      "|i was an nba wife...|i felt like i was...|\n",
      "|how to get rid of...|they seem harmles...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TEXT CLEANING ###\n",
    "\n",
    "# Define a function to clean text (remove special characters and convert to lowercase)\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        return text.lower().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "\n",
    "# Apply text cleaning to Title and Abstract\n",
    "news_df = news_df.withColumn(\"CleanTitle\", clean_text_udf(col(\"Title\")))\n",
    "news_df = news_df.withColumn(\"CleanAbstract\", clean_text_udf(col(\"Abstract\")))\n",
    "\n",
    "# Display cleaned text\n",
    "news_df.select(\"CleanTitle\", \"CleanAbstract\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         TitleTokens|      AbstractTokens|\n",
      "+--------------------+--------------------+\n",
      "|[the, brands, que...|[shop, the, noteb...|\n",
      "|[50, worst, habit...|[these, seemingly...|\n",
      "|[the, cost, of, t...|[lt., ivan, molch...|\n",
      "|[i, was, an, nba,...|[i, felt, like, i...|\n",
      "|[how, to, get, ri...|[they, seem, harm...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TOKENIZATION ###\n",
    "\n",
    "# Tokenize CleanTitle and CleanAbstract\n",
    "tokenizer_title = Tokenizer(inputCol=\"CleanTitle\", outputCol=\"TitleTokens\")\n",
    "tokenizer_abstract = Tokenizer(inputCol=\"CleanAbstract\", outputCol=\"AbstractTokens\")\n",
    "\n",
    "news_df = tokenizer_title.transform(news_df)\n",
    "news_df = tokenizer_abstract.transform(news_df)\n",
    "\n",
    "# Display tokenized data\n",
    "news_df.select(\"TitleTokens\", \"AbstractTokens\").show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "| FilteredTitleTokens|FilteredAbstractTokens|\n",
      "+--------------------+----------------------+\n",
      "|[brands, queen, e...|  [shop, notebooks,...|\n",
      "|[50, worst, habit...|  [seemingly, harml...|\n",
      "|[cost, trump's, a...|  [lt., ivan, molch...|\n",
      "|[nba, wife., affe...|  [felt, like, frau...|\n",
      "|[get, rid, skin, ...|  [seem, harmless,,...|\n",
      "+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### STOPWORDS REMOVAL ###\n",
    "\n",
    "# Remove stopwords from TitleTokens and AbstractTokens\n",
    "stopword_remover_title = StopWordsRemover(inputCol=\"TitleTokens\", outputCol=\"FilteredTitleTokens\")\n",
    "stopword_remover_abstract = StopWordsRemover(inputCol=\"AbstractTokens\", outputCol=\"FilteredAbstractTokens\")\n",
    "\n",
    "news_df = stopword_remover_title.transform(news_df)\n",
    "news_df = stopword_remover_abstract.transform(news_df)\n",
    "\n",
    "# Display filtered tokens\n",
    "news_df.select(\"FilteredTitleTokens\", \"FilteredAbstractTokens\").show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to clean each token in the array\n",
    "def clean_tokens(tokens):\n",
    "    if tokens:\n",
    "        return [token.replace(\",\", \"\") for token in tokens]  # Remove commas\n",
    "    return tokens\n",
    "\n",
    "# Register the UDF\n",
    "clean_tokens_udf = udf(clean_tokens, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to FilteredTitleTokens\n",
    "news_df = news_df.withColumn(\"FilteredTitleTokens\", clean_tokens_udf(col(\"FilteredTitleTokens\")))\n",
    "news_df = news_df.withColumn(\"FilteredAbstractTokens\", clean_tokens_udf(col(\"FilteredAbstractTokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               Title| FilteredTitleTokens|\n",
      "+--------------------+--------------------+\n",
      "|The Brands Queen ...|[brands, queen, e...|\n",
      "|50 Worst Habits F...|[50, worst, habit...|\n",
      "|The Cost of Trump...|[cost, trump's, a...|\n",
      "|I Was An NBA Wife...|[nba, wife., affe...|\n",
      "|How to Get Rid of...|[get, rid, skin, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"Title\", \"FilteredTitleTokens\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|            Abstract|FilteredAbstractTokens|\n",
      "+--------------------+----------------------+\n",
      "|Shop the notebook...|  [shop, notebooks,...|\n",
      "|These seemingly h...|  [seemingly, harml...|\n",
      "+--------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"Abstract\", \"FilteredAbstractTokens\").show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities',\n",
       " 'CleanTitle',\n",
       " 'CleanAbstract',\n",
       " 'TitleTokens',\n",
       " 'AbstractTokens',\n",
       " 'FilteredTitleTokens',\n",
       " 'FilteredAbstractTokens']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|AbstractEntities|          CleanTitle|       CleanAbstract|         TitleTokens|      AbstractTokens| FilteredTitleTokens|FilteredAbstractTokens|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|              []|the brands queen ...|shop the notebook...|[the, brands, que...|[shop, the, noteb...|[brands, queen, e...|  [shop, notebooks,...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EMBEDDINGS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      CombinedTokens|\n",
      "+--------------------+\n",
      "|brands queen eliz...|\n",
      "|50 worst habits b...|\n",
      "|cost trump's aid ...|\n",
      "|nba wife. affecte...|\n",
      "|get rid skin tags...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine tokens \n",
    "news_df = news_df.withColumn(\"CombinedTokens\", concat_ws(\" \", \"FilteredTitleTokens\", \"FilteredAbstractTokens\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "news_df.select(\"CombinedTokens\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CombinedTokens (string) into an array of tokens\n",
    "news_df = news_df.withColumn(\"CombinedWords\", split(news_df[\"CombinedTokens\"], \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|NewsID|         RawFeatures|\n",
      "+------+--------------------+\n",
      "|N55528|(10000,[312,978,1...|\n",
      "|N19639|(10000,[27,438,85...|\n",
      "|N61837|(10000,[63,176,35...|\n",
      "|N53526|(10000,[38,89,230...|\n",
      "|N38324|(10000,[6,17,20,1...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute term frequency (TF) using CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"CombinedWords\", outputCol=\"RawFeatures\", vocabSize=10000, minDF=2)\n",
    "cv_model = cv.fit(news_df)\n",
    "news_df = cv_model.transform(news_df)\n",
    "\n",
    "# Show the resulting term frequency vector\n",
    "news_df.select(\"NewsID\", \"RawFeatures\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|NewsID|         RawFeatures|\n",
      "+------+--------------------+\n",
      "|N55528|(10000,[312,978,1...|\n",
      "|N19639|(10000,[27,438,85...|\n",
      "|N61837|(10000,[63,176,35...|\n",
      "|N53526|(10000,[38,89,230...|\n",
      "|N38324|(10000,[6,17,20,1...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"RawFeatures\", outputCol=\"TFIDFeatures\")\n",
    "idf_model = idf.fit(news_df)\n",
    "news_df = idf_model.transform(news_df)\n",
    "\n",
    "# Show the resulting DataFrame with TF-IDF features\n",
    "news_df.select(\"NewsID\", \"RawFeatures\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT EMBEDDING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPORT PROCESSED DATA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities',\n",
       " 'CleanTitle',\n",
       " 'CleanAbstract',\n",
       " 'TitleTokens',\n",
       " 'AbstractTokens',\n",
       " 'FilteredTitleTokens',\n",
       " 'FilteredAbstractTokens',\n",
       " 'CombinedTokens',\n",
       " 'CombinedWords',\n",
       " 'RawFeatures',\n",
       " 'TFIDFeatures']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB: example (10000, [310, 978, 1119], [0.5, 1.2, 0.8])\n",
    "#This means:\n",
    "#The vocabulary size is 10,000.\n",
    "#Words at indices 310, 978, and 1119 in the vocabulary are present in the document.\n",
    "#Their respective TF-IDF scores are 0.5, 1.2, and 0.8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/09 16:42:30 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Save DataFrame to CSV\n",
    "news_df.write.parquet(\"news\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID|Category|        Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|          CleanTitle|       CleanAbstract|         TitleTokens|      AbstractTokens| FilteredTitleTokens|FilteredAbstractTokens|      CombinedTokens|       CombinedWords|         RawFeatures|        TFIDFeatures|\n",
      "+------+--------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| N5727|  sports|football_nfl_videos|Every disruptive ...|The New England P...|https://assets.ms...|[{\"Label\": \"New E...|[{\"Label\": \"New E...|every disruptive ...|the new england p...|[every, disruptiv...|[the, new, englan...|[every, disruptiv...|  [new, england, pa...|every disruptive ...|[every, disruptiv...|(10000,[1,9,87,90...|(10000,[1,9,87,90...|\n",
      "+------+--------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the saved Parquet file\n",
    "parquet_path = \"news\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "tfidf_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Show the schema of the loaded DataFrame\n",
    "tfidf_df.show(1, truncate = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
