{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec0f982-68d8-4fc4-8c40-f3841a9c9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, udf, regexp_replace, lit, from_unixtime\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType, StringType, MapType\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, MapType, StringType\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import split, explode, regexp_extract, col, collect_list, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, FloatType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a502793-bc5f-4e96-a753-22bfb658195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/09 15:58:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/09 15:58:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/09 15:58:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Stop existing Spark context if any\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Reinitialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BehaviorsProcessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111b3241-5686-4261-95c0-751debc6b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"  # Replace with your Java path\n",
    "os.environ[\"SPARK_HOME\"] = \"/path/to/spark\"  # Replace with your Spark installation path\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"  # Replace with your Python path\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9448e8fd-8103-4d5e-b2d8-8cdc939e959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ImpressionID: string (nullable = true)\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- History: string (nullable = true)\n",
      " |-- Impressions: string (nullable = true)\n",
      "\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "|ImpressionID|UserID|Time                 |History                                                       |Impressions      |\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "|1           |U13740|11/11/2019 9:05:58 AM|N55189 N42782 N34694 N45794 N18445 N63302 N10414 N19347 N31801|N55689-1 N35729-0|\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "behaviors_schema = StructType([\n",
    "    StructField(\"ImpressionID\", StringType(), True),\n",
    "    StructField(\"UserID\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"History\", StringType(), True),\n",
    "    StructField(\"Impressions\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the behaviors.tsv file\n",
    "behaviors_df = spark.read.csv(\n",
    "    \"data/mind/MINDsmall_train/behaviors.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    schema=behaviors_schema,\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Display the schema and a sample row\n",
    "behaviors_df.printSchema()\n",
    "behaviors_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3137302-b6af-48b3-922a-e71c88a1cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------------------------------------------------------------------+\n",
      "|ImpressionID|UserID|HistoryList                                                             |\n",
      "+------------+------+------------------------------------------------------------------------+\n",
      "|1           |U13740|[N55189, N42782, N34694, N45794, N18445, N63302, N10414, N19347, N31801]|\n",
      "+------------+------+------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split History into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"HistoryList\", split(col(\"History\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"History\")  # Drop original History column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"UserID\", \"HistoryList\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50637a47-9a55-47c5-9898-deedc22f558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|ImpressionID|ImpressionsList     |\n",
      "+------------+--------------------+\n",
      "|1           |[N55689-1, N35729-0]|\n",
      "+------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split Impressions into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"ImpressionsList\", split(col(\"Impressions\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"Impressions\")  # Drop original Impressions column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"ImpressionsList\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f03662b-9d9e-4055-875f-af051a5f7d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------+----------+\n",
      "|ImpressionID|UserID|CandidateNewsID|ClickLabel|\n",
      "+------------+------+---------------+----------+\n",
      "|1           |U13740|N55689         |1         |\n",
      "|1           |U13740|N35729         |0         |\n",
      "|2           |U91836|N20678         |0         |\n",
      "|2           |U91836|N39317         |0         |\n",
      "|2           |U91836|N58114         |0         |\n",
      "+------------+------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode ImpressionsList\n",
    "impressions_exploded = behaviors_df.select(\n",
    "    \"ImpressionID\",\n",
    "    \"UserID\",\n",
    "    \"Time\",\n",
    "    \"HistoryList\",\n",
    "    explode(\"ImpressionsList\").alias(\"ImpressionItem\")\n",
    ")\n",
    "\n",
    "# Extract CandidateNewsID and ClickLabel using regex\n",
    "impressions_exploded = impressions_exploded \\\n",
    "    .withColumn(\"CandidateNewsID\", regexp_extract(col(\"ImpressionItem\"), r\"^(N\\d+)-\\d+$\", 1)) \\\n",
    "    .withColumn(\"ClickLabel\", regexp_extract(col(\"ImpressionItem\"), r\"^N\\d+-(\\d+)$\", 1).cast(\"integer\")) \\\n",
    "    .drop(\"ImpressionItem\")\n",
    "\n",
    "# Verify the transformation\n",
    "impressions_exploded.select(\"ImpressionID\", \"UserID\", \"CandidateNewsID\", \"ClickLabel\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eef02ba-8117-4d3b-a442-d86f7df1406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tfidf_path = \"news\"\n",
    "news_features_df = spark.read.parquet(news_tfidf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244ebec2-1d16-4446-b3f5-e3e65762be40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities',\n",
       " 'CleanTitle',\n",
       " 'CleanAbstract',\n",
       " 'TitleTokens',\n",
       " 'AbstractTokens',\n",
       " 'FilteredTitleTokens',\n",
       " 'FilteredAbstractTokens',\n",
       " 'CombinedTokens',\n",
       " 'CombinedWords',\n",
       " 'RawFeatures',\n",
       " 'TFIDFeatures']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809a45c8-3fdf-4859-ae0f-f2a258b1f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join impressions with news_features_df on CandidateNewsID\n",
    "impressions_with_features = impressions_exploded.join(\n",
    "    news_features_df,\n",
    "    impressions_exploded.CandidateNewsID == news_features_df.NewsID,\n",
    "    how=\"left\"\n",
    ").drop(news_features_df.NewsID)  # Drop duplicate NewsID column if present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ed1d5-faf6-48b4-8973-1bcc35dfa673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the join\n",
    "impressions_with_features.select(\"ImpressionID\", \"UserID\", \"CandidateNewsID\", \"ClickLabel\", \"TFIDFeatures\", \"Category\").show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c87016-1cd3-4dad-b5a5-b2214b1300c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records where ClickLabel == 1\n",
    "clicked_news_df = impressions_with_features.filter(col(\"ClickLabel\") == 1)\n",
    "\n",
    "# Verify the filtered DataFrame\n",
    "clicked_news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb62de2-2179-4ca1-a57a-bda89fbef94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define a function to compute cosine similarity\n",
    "def cosine_similarity_udf(u_vec, i_vec):\n",
    "    if u_vec is None or i_vec is None:\n",
    "        return 0.0\n",
    "    u_arr = u_vec.toArray()\n",
    "    i_arr = i_vec.toArray()\n",
    "    norm_u = np.linalg.norm(u_arr)\n",
    "    norm_i = np.linalg.norm(i_arr)\n",
    "    if norm_u == 0 or norm_i == 0:  # Avoid division by zero\n",
    "        return 0.0\n",
    "    return float(np.dot(u_arr, i_arr) / (norm_u * norm_i))\n",
    "\n",
    "# Register the UDF\n",
    "cosine_udf = udf(cosine_similarity_udf, FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434a104-0ad9-469a-bce7-1348a7459e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Define a UDF to average vectors\n",
    "def average_vectors(vectors, count):\n",
    "    if count == 0 or not vectors:  # Avoid division by zero or empty vectors\n",
    "        return Vectors.dense([0.0] * len(vectors[0].toArray())) if vectors else Vectors.dense([0.0])\n",
    "    summed_vector = [sum(component) for component in zip(*[v.toArray() for v in vectors])]\n",
    "    averaged_vector = [component / count for component in summed_vector]\n",
    "    return Vectors.dense(averaged_vector)\n",
    "\n",
    "avg_vector_udf = udf(average_vectors, VectorUDT())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974faeb2-124d-4332-aea8-890c1de0c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Build category-specific user profiles\n",
    "category_user_profiles = clicked_news_df.groupBy(\"UserID\", \"Category\").agg(\n",
    "    F.collect_list(\"TFIDFeatures\").alias(\"UserVectors\"),\n",
    "    F.size(F.collect_list(\"TFIDFeatures\")).alias(\"CountVectors\")\n",
    ").withColumn(\n",
    "    \"UserProfile\",\n",
    "    avg_vector_udf(\"UserVectors\", \"CountVectors\")\n",
    ").drop(\"UserVectors\", \"CountVectors\")\n",
    "\n",
    "# Join news data with user profiles to ensure category alignment\n",
    "user_recs_with_category = category_user_profiles.join(\n",
    "    news_features_df,  # news DataFrame with category information\n",
    "    on=\"Category\",  # Match articles with user preferences in the same category\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55208754-3bd8-4570-9416-2706973fb627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o263.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m top_n_recs \u001b[38;5;241m=\u001b[39m user_recs_with_category\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrow_number()\u001b[38;5;241m.\u001b[39mover(windowSpec)) \\\n\u001b[1;32m     18\u001b[0m                                     \u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Top-5 recommendations per category\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtop_n_recs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUserID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNewsID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimilarity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:615\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncate=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be either bool or int.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(truncate)\n\u001b[1;32m    613\u001b[0m     )\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o263.showString"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity scores\n",
    "def cosine_similarity(u_vec, i_vec):\n",
    "    u_arr = u_vec.toArray()\n",
    "    i_arr = i_vec.toArray()\n",
    "    similarity = np.dot(u_arr, i_arr) / (np.linalg.norm(u_arr) * np.linalg.norm(i_arr))\n",
    "    return float(similarity)\n",
    "\n",
    "cosine_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "user_recs_with_category = user_recs_with_category.withColumn(\n",
    "    \"similarity\",\n",
    "    cosine_udf(F.col(\"UserProfile\"), F.col(\"TFIDFeatures\"))\n",
    ")\n",
    "\n",
    "# Rank and select top recommendations\n",
    "windowSpec = Window.partitionBy(\"UserID\", \"Category\").orderBy(F.desc(\"similarity\"))\n",
    "top_n_recs = user_recs_with_category.withColumn(\"rank\", F.row_number().over(windowSpec)) \\\n",
    "                                    .filter(F.col(\"rank\") <= 5)  # Top-5 recommendations per category\n",
    "\n",
    "# Display results\n",
    "top_n_recs.select(\"UserID\", \"Category\", \"NewsID\", \"similarity\", \"rank\").show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
