{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/09 15:16:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MIND Dataset Processing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|                  []|\n",
      "|N19639|   health|     weightloss|50 Worst Habits F...|These seemingly h...|https://assets.ms...|[{\"Label\": \"Adipo...|[{\"Label\": \"Adipo...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the news.tsv file\n",
    "news_path = \"data/mind/MINDsmall_train/news.tsv\"\n",
    "\n",
    "# Define column names for the news.tsv file\n",
    "news_columns = [\"NewsID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"]\n",
    "\n",
    "# Load the news.tsv file into a Spark DataFrame\n",
    "news_df = spark.read.csv(\n",
    "    news_path,\n",
    "    sep=\"\\t\",\n",
    "    schema=\"NewsID STRING, Category STRING, Subcategory STRING, Title STRING, Abstract STRING, URL STRING, TitleEntities STRING, AbstractEntities STRING\",\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Assign column names\n",
    "news_df.show(n=2, truncate=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PRE PROCESSING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, length, size, udf\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/09 15:16:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|                  []|\n",
      "|N19639|   health|     weightloss|50 Worst Habits F...|These seemingly h...|https://assets.ms...|[{\"Label\": \"Adipo...|[{\"Label\": \"Adipo...|\n",
      "|N61837|     news|      newsworld|The Cost of Trump...|Lt. Ivan Molchane...|https://assets.ms...|                  []|[{\"Label\": \"Ukrai...|\n",
      "|N53526|   health|         voices|I Was An NBA Wife...|I felt like I was...|https://assets.ms...|                  []|[{\"Label\": \"Natio...|\n",
      "|N38324|   health|        medical|How to Get Rid of...|They seem harmles...|https://assets.ms...|[{\"Label\": \"Skin ...|[{\"Label\": \"Skin ...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PreprocessingPipeline\").getOrCreate()\n",
    "\n",
    "# Load your data (modify the path as necessary)\n",
    "news_df = spark.read.csv(\"data/mind/MINDsmall_train/news.tsv\", sep=\"\\t\", header=False, inferSchema=True)\n",
    "\n",
    "# Assign column names\n",
    "news_df = news_df.toDF(\"NewsID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\")\n",
    "\n",
    "# Display initial rows\n",
    "news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping missing values: 48616\n"
     ]
    }
   ],
   "source": [
    "### MISSING VALUES ###\n",
    "\n",
    "# Drop rows where Title or Abstract are missing\n",
    "news_df = news_df.na.drop(subset=[\"Title\", \"Abstract\"])\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Rows after dropping missing values: {news_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          CleanTitle|       CleanAbstract|\n",
      "+--------------------+--------------------+\n",
      "|the brands queen ...|shop the notebook...|\n",
      "|50 worst habits f...|these seemingly h...|\n",
      "|the cost of trump...|lt. ivan molchane...|\n",
      "|i was an nba wife...|i felt like i was...|\n",
      "|how to get rid of...|they seem harmles...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TEXT CLEANING ###\n",
    "\n",
    "# Define a function to clean text (remove special characters and convert to lowercase)\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        return text.lower().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "\n",
    "# Apply text cleaning to Title and Abstract\n",
    "news_df = news_df.withColumn(\"CleanTitle\", clean_text_udf(col(\"Title\")))\n",
    "news_df = news_df.withColumn(\"CleanAbstract\", clean_text_udf(col(\"Abstract\")))\n",
    "\n",
    "# Display cleaned text\n",
    "news_df.select(\"CleanTitle\", \"CleanAbstract\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         TitleTokens|      AbstractTokens|\n",
      "+--------------------+--------------------+\n",
      "|[the, brands, que...|[shop, the, noteb...|\n",
      "|[50, worst, habit...|[these, seemingly...|\n",
      "|[the, cost, of, t...|[lt., ivan, molch...|\n",
      "|[i, was, an, nba,...|[i, felt, like, i...|\n",
      "|[how, to, get, ri...|[they, seem, harm...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TOKENIZATION ###\n",
    "\n",
    "# Tokenize CleanTitle and CleanAbstract\n",
    "tokenizer_title = Tokenizer(inputCol=\"CleanTitle\", outputCol=\"TitleTokens\")\n",
    "tokenizer_abstract = Tokenizer(inputCol=\"CleanAbstract\", outputCol=\"AbstractTokens\")\n",
    "\n",
    "news_df = tokenizer_title.transform(news_df)\n",
    "news_df = tokenizer_abstract.transform(news_df)\n",
    "\n",
    "# Display tokenized data\n",
    "news_df.select(\"TitleTokens\", \"AbstractTokens\").show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "| FilteredTitleTokens|FilteredAbstractTokens|\n",
      "+--------------------+----------------------+\n",
      "|[brands, queen, e...|  [shop, notebooks,...|\n",
      "|[50, worst, habit...|  [seemingly, harml...|\n",
      "|[cost, trump's, a...|  [lt., ivan, molch...|\n",
      "|[nba, wife., affe...|  [felt, like, frau...|\n",
      "|[get, rid, skin, ...|  [seem, harmless,,...|\n",
      "+--------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### STOPWORDS REMOVAL ###\n",
    "\n",
    "# Remove stopwords from TitleTokens and AbstractTokens\n",
    "stopword_remover_title = StopWordsRemover(inputCol=\"TitleTokens\", outputCol=\"FilteredTitleTokens\")\n",
    "stopword_remover_abstract = StopWordsRemover(inputCol=\"AbstractTokens\", outputCol=\"FilteredAbstractTokens\")\n",
    "\n",
    "news_df = stopword_remover_title.transform(news_df)\n",
    "news_df = stopword_remover_abstract.transform(news_df)\n",
    "\n",
    "# Display filtered tokens\n",
    "news_df.select(\"FilteredTitleTokens\", \"FilteredAbstractTokens\").show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to clean each token in the array\n",
    "def clean_tokens(tokens):\n",
    "    if tokens:\n",
    "        return [token.replace(\",\", \"\") for token in tokens]  # Remove commas\n",
    "    return tokens\n",
    "\n",
    "# Register the UDF\n",
    "clean_tokens_udf = udf(clean_tokens, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to FilteredTitleTokens\n",
    "news_df = news_df.withColumn(\"FilteredTitleTokens\", clean_tokens_udf(col(\"FilteredTitleTokens\")))\n",
    "news_df = news_df.withColumn(\"FilteredAbstractTokens\", clean_tokens_udf(col(\"FilteredAbstractTokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               Title| FilteredTitleTokens|\n",
      "+--------------------+--------------------+\n",
      "|The Brands Queen ...|[brands, queen, e...|\n",
      "|50 Worst Habits F...|[50, worst, habit...|\n",
      "|The Cost of Trump...|[cost, trump's, a...|\n",
      "|I Was An NBA Wife...|[nba, wife., affe...|\n",
      "|How to Get Rid of...|[get, rid, skin, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"Title\", \"FilteredTitleTokens\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|            Abstract|FilteredAbstractTokens|\n",
      "+--------------------+----------------------+\n",
      "|Shop the notebook...|  [shop, notebooks,...|\n",
      "|These seemingly h...|  [seemingly, harml...|\n",
      "+--------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"Abstract\", \"FilteredAbstractTokens\").show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities',\n",
       " 'CleanTitle',\n",
       " 'CleanAbstract',\n",
       " 'TitleTokens',\n",
       " 'AbstractTokens',\n",
       " 'FilteredTitleTokens',\n",
       " 'FilteredAbstractTokens']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|AbstractEntities|          CleanTitle|       CleanAbstract|         TitleTokens|      AbstractTokens| FilteredTitleTokens|FilteredAbstractTokens|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|              []|the brands queen ...|shop the notebook...|[the, brands, que...|[shop, the, noteb...|[brands, queen, e...|  [shop, notebooks,...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EMBEDDINGS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      CombinedTokens|\n",
      "+--------------------+\n",
      "|brands queen eliz...|\n",
      "|50 worst habits b...|\n",
      "|cost trump's aid ...|\n",
      "|nba wife. affecte...|\n",
      "|get rid skin tags...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine tokens \n",
    "news_df = news_df.withColumn(\"CombinedTokens\", concat_ws(\" \", \"FilteredTitleTokens\", \"FilteredAbstractTokens\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "news_df.select(\"CombinedTokens\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CombinedTokens (string) into an array of tokens\n",
    "news_df = news_df.withColumn(\"CombinedWords\", split(news_df[\"CombinedTokens\"], \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|NewsID|         RawFeatures|\n",
      "+------+--------------------+\n",
      "|N55528|(10000,[309,978,1...|\n",
      "|N19639|(10000,[27,438,85...|\n",
      "|N61837|(10000,[63,176,35...|\n",
      "|N53526|(10000,[38,89,231...|\n",
      "|N38324|(10000,[6,17,20,1...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute term frequency (TF) using CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"CombinedWords\", outputCol=\"RawFeatures\", vocabSize=10000, minDF=2)\n",
    "cv_model = cv.fit(news_df)\n",
    "news_df = cv_model.transform(news_df)\n",
    "\n",
    "# Show the resulting term frequency vector\n",
    "news_df.select(\"NewsID\", \"RawFeatures\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|NewsID|         RawFeatures|\n",
      "+------+--------------------+\n",
      "|N55528|(10000,[309,978,1...|\n",
      "|N19639|(10000,[27,438,85...|\n",
      "|N61837|(10000,[63,176,35...|\n",
      "|N53526|(10000,[38,89,231...|\n",
      "|N38324|(10000,[6,17,20,1...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"RawFeatures\", outputCol=\"TFIDFeatures\")\n",
    "idf_model = idf.fit(news_df)\n",
    "news_df = idf_model.transform(news_df)\n",
    "\n",
    "# Show the resulting DataFrame with TF-IDF features\n",
    "news_df.select(\"NewsID\", \"RawFeatures\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT EMBEDDING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparknlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark NLP version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparknlp\u001b[38;5;241m.\u001b[39mversion()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Example output: 4.4.3\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparknlp'"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")  # Example output: 4.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "24/12/09 15:10:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Spark NLP\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m      8\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, this is a test sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m     (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark NLP is great!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define the DocumentAssembler\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m document_assembler \u001b[38;5;241m=\u001b[39m \u001b[43mDocumentAssembler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define the BERT Sentence Embeddings\u001b[39;00m\n\u001b[1;32m     18\u001b[0m bert_embedder \u001b[38;5;241m=\u001b[39m BertSentenceEmbeddings\u001b[38;5;241m.\u001b[39mpretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_small_bert_L2_128\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sparknlp/base/document_assembler.py:96\u001b[0m, in \u001b[0;36mDocumentAssembler.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDocumentAssembler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.DocumentAssembler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, cleanupMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sparknlp/internal/annotator_transformer.py:36\u001b[0m, in \u001b[0;36mAnnotatorTransformer.__init__\u001b[0;34m(self, classname)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import BertSentenceEmbeddings\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Sample DataFrame\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"Hello, this is a test sentence.\"),\n",
    "    (2, \"Spark NLP is great!\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Define the DocumentAssembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Define the BERT Sentence Embeddings\n",
    "bert_embedder = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\", \"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"bert_embeddings\")\n",
    "\n",
    "# Build the Pipeline\n",
    "pipeline = Pipeline(stages=[document_assembler, bert_embedder])\n",
    "\n",
    "# Fit and transform the data\n",
    "model = pipeline.fit(data)\n",
    "result = model.transform(data)\n",
    "\n",
    "# Show the Results\n",
    "result.select(\"bert_embeddings\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities',\n",
       " 'CleanTitle',\n",
       " 'CleanAbstract',\n",
       " 'TitleTokens',\n",
       " 'AbstractTokens',\n",
       " 'FilteredTitleTokens',\n",
       " 'FilteredAbstractTokens',\n",
       " 'CombinedTokens',\n",
       " 'CombinedWords',\n",
       " 'RawFeatures',\n",
       " 'TFIDFeatures']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB: example (10000, [310, 978, 1119], [0.5, 1.2, 0.8])\n",
    "#This means:\n",
    "#The vocabulary size is 10,000.\n",
    "#Words at indices 310, 978, and 1119 in the vocabulary are present in the document.\n",
    "#Their respective TF-IDF scores are 0.5, 1.2, and 0.8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/09 14:22:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Save DataFrame to CSV\n",
    "news_df.write.parquet(\"news\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID|Category|        Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|          CleanTitle|       CleanAbstract|         TitleTokens|      AbstractTokens| FilteredTitleTokens|FilteredAbstractTokens|      CombinedTokens|       CombinedWords|         RawFeatures|        TFIDFeatures|\n",
      "+------+--------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| N5727|  sports|football_nfl_videos|Every disruptive ...|The New England P...|https://assets.ms...|[{\"Label\": \"New E...|[{\"Label\": \"New E...|every disruptive ...|the new england p...|[every, disruptiv...|[the, new, englan...|[every, disruptiv...|  [new, england, pa...|every disruptive ...|[every, disruptiv...|(10000,[1,9,87,90...|(10000,[1,9,87,90...|\n",
      "+------+--------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the saved Parquet file\n",
    "parquet_path = \"news\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "tfidf_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Show the schema of the loaded DataFrame\n",
    "tfidf_df.show(1, truncate = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
