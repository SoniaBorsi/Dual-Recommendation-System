{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec0f982-68d8-4fc4-8c40-f3841a9c9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, udf, regexp_replace, lit, from_unixtime\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType, StringType, MapType\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, MapType, StringType\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import split, explode, regexp_extract, col, collect_list, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, FloatType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a502793-bc5f-4e96-a753-22bfb658195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/09 15:58:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/09 15:58:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/09 15:58:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Stop existing Spark context if any\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Reinitialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BehaviorsProcessing\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111b3241-5686-4261-95c0-751debc6b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"  # Replace with your Java path\n",
    "os.environ[\"SPARK_HOME\"] = \"/path/to/spark\"  # Replace with your Spark installation path\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"  # Replace with your Python path\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9448e8fd-8103-4d5e-b2d8-8cdc939e959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ImpressionID: string (nullable = true)\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- History: string (nullable = true)\n",
      " |-- Impressions: string (nullable = true)\n",
      "\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "|ImpressionID|UserID|Time                 |History                                                       |Impressions      |\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "|1           |U13740|11/11/2019 9:05:58 AM|N55189 N42782 N34694 N45794 N18445 N63302 N10414 N19347 N31801|N55689-1 N35729-0|\n",
      "+------------+------+---------------------+--------------------------------------------------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "behaviors_schema = StructType([\n",
    "    StructField(\"ImpressionID\", StringType(), True),\n",
    "    StructField(\"UserID\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"History\", StringType(), True),\n",
    "    StructField(\"Impressions\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the behaviors.tsv file\n",
    "behaviors_df = spark.read.csv(\n",
    "    \"data/mind/MINDsmall_train/behaviors.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    schema=behaviors_schema,\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Display the schema and a sample row\n",
    "behaviors_df.printSchema()\n",
    "behaviors_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3137302-b6af-48b3-922a-e71c88a1cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------------------------------------------------------------------+\n",
      "|ImpressionID|UserID|HistoryList                                                             |\n",
      "+------------+------+------------------------------------------------------------------------+\n",
      "|1           |U13740|[N55189, N42782, N34694, N45794, N18445, N63302, N10414, N19347, N31801]|\n",
      "+------------+------+------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split History into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"HistoryList\", split(col(\"History\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"History\")  # Drop original History column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"UserID\", \"HistoryList\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50637a47-9a55-47c5-9898-deedc22f558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|ImpressionID|ImpressionsList     |\n",
      "+------------+--------------------+\n",
      "|1           |[N55689-1, N35729-0]|\n",
      "+------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split Impressions into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"ImpressionsList\", split(col(\"Impressions\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"Impressions\")  # Drop original Impressions column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"ImpressionsList\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f03662b-9d9e-4055-875f-af051a5f7d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------+----------+\n",
      "|ImpressionID|UserID|CandidateNewsID|ClickLabel|\n",
      "+------------+------+---------------+----------+\n",
      "|1           |U13740|N55689         |1         |\n",
      "|1           |U13740|N35729         |0         |\n",
      "|2           |U91836|N20678         |0         |\n",
      "|2           |U91836|N39317         |0         |\n",
      "|2           |U91836|N58114         |0         |\n",
      "+------------+------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode ImpressionsList\n",
    "impressions_exploded = behaviors_df.select(\n",
    "    \"ImpressionID\",\n",
    "    \"UserID\",\n",
    "    \"Time\",\n",
    "    \"HistoryList\",\n",
    "    explode(\"ImpressionsList\").alias(\"ImpressionItem\")\n",
    ")\n",
    "\n",
    "# Extract CandidateNewsID and ClickLabel using regex\n",
    "impressions_exploded = impressions_exploded \\\n",
    "    .withColumn(\"CandidateNewsID\", regexp_extract(col(\"ImpressionItem\"), r\"^(N\\d+)-\\d+$\", 1)) \\\n",
    "    .withColumn(\"ClickLabel\", regexp_extract(col(\"ImpressionItem\"), r\"^N\\d+-(\\d+)$\", 1).cast(\"integer\")) \\\n",
    "    .drop(\"ImpressionItem\")\n",
    "\n",
    "# Verify the transformation\n",
    "impressions_exploded.select(\"ImpressionID\", \"UserID\", \"CandidateNewsID\", \"ClickLabel\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eef02ba-8117-4d3b-a442-d86f7df1406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tfidf_path = \"news\"\n",
    "news_features_df = spark.read.parquet(news_tfidf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244ebec2-1d16-4446-b3f5-e3e65762be40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities',\n",
       " 'CleanTitle',\n",
       " 'CleanAbstract',\n",
       " 'TitleTokens',\n",
       " 'AbstractTokens',\n",
       " 'FilteredTitleTokens',\n",
       " 'FilteredAbstractTokens',\n",
       " 'CombinedTokens',\n",
       " 'CombinedWords',\n",
       " 'RawFeatures',\n",
       " 'TFIDFeatures']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809a45c8-3fdf-4859-ae0f-f2a258b1f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join impressions with news_features_df on CandidateNewsID\n",
    "impressions_with_features = impressions_exploded.join(\n",
    "    news_features_df,\n",
    "    impressions_exploded.CandidateNewsID == news_features_df.NewsID,\n",
    "    how=\"left\"\n",
    ").drop(news_features_df.NewsID)  # Drop duplicate NewsID column if present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ed1d5-faf6-48b4-8973-1bcc35dfa673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the join\n",
    "impressions_with_features.select(\"ImpressionID\", \"UserID\", \"CandidateNewsID\", \"ClickLabel\", \"TFIDFeatures\", \"Category\").show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c87016-1cd3-4dad-b5a5-b2214b1300c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records where ClickLabel == 1\n",
    "clicked_news_df = impressions_with_features.filter(col(\"ClickLabel\") == 1)\n",
    "\n",
    "# Verify the filtered DataFrame\n",
    "clicked_news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb62de2-2179-4ca1-a57a-bda89fbef94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define a function to compute cosine similarity\n",
    "def cosine_similarity_udf(u_vec, i_vec):\n",
    "    if u_vec is None or i_vec is None:\n",
    "        return 0.0\n",
    "    u_arr = u_vec.toArray()\n",
    "    i_arr = i_vec.toArray()\n",
    "    norm_u = np.linalg.norm(u_arr)\n",
    "    norm_i = np.linalg.norm(i_arr)\n",
    "    if norm_u == 0 or norm_i == 0:  # Avoid division by zero\n",
    "        return 0.0\n",
    "    return float(np.dot(u_arr, i_arr) / (norm_u * norm_i))\n",
    "\n",
    "# Register the UDF\n",
    "cosine_udf = udf(cosine_similarity_udf, FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434a104-0ad9-469a-bce7-1348a7459e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# Define a UDF to average vectors\n",
    "def average_vectors(vectors, count):\n",
    "    if count == 0 or not vectors:  # Avoid division by zero or empty vectors\n",
    "        return Vectors.dense([0.0] * len(vectors[0].toArray())) if vectors else Vectors.dense([0.0])\n",
    "    summed_vector = [sum(component) for component in zip(*[v.toArray() for v in vectors])]\n",
    "    averaged_vector = [component / count for component in summed_vector]\n",
    "    return Vectors.dense(averaged_vector)\n",
    "\n",
    "avg_vector_udf = udf(average_vectors, VectorUDT())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974faeb2-124d-4332-aea8-890c1de0c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Build category-specific user profiles\n",
    "category_user_profiles = clicked_news_df.groupBy(\"UserID\", \"Category\").agg(\n",
    "    F.collect_list(\"TFIDFeatures\").alias(\"UserVectors\"),\n",
    "    F.size(F.collect_list(\"TFIDFeatures\")).alias(\"CountVectors\")\n",
    ").withColumn(\n",
    "    \"UserProfile\",\n",
    "    avg_vector_udf(\"UserVectors\", \"CountVectors\")\n",
    ").drop(\"UserVectors\", \"CountVectors\")\n",
    "\n",
    "# Join news data with user profiles to ensure category alignment\n",
    "user_recs_with_category = category_user_profiles.join(\n",
    "    news_features_df,  # news DataFrame with category information\n",
    "    on=\"Category\",  # Match articles with user preferences in the same category\n",
    "    how=\"inner\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
