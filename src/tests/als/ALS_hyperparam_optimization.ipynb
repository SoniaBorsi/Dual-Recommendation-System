{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "from src.utilities.data_utils import preprocess_behaviors_mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train.zip\"\n",
    "validation_path = \"/home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_dev.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/26 19:48:46 WARN Utils: Your hostname, DESKTOP-LQJ6T08 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/26 19:48:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/26 19:48:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark session initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ALS Hyperparameter Tuning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema for loading the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"impression_id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"history\", StringType(), True),\n",
    "    StructField(\"impressions\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract and preprocess data\n",
    "def extract_and_load_zip(file_path, schema):\n",
    "    # Create a temporary directory for extraction\n",
    "    extracted_path = os.path.splitext(file_path)[0]\n",
    "    if not os.path.exists(extracted_path):\n",
    "        print(f\"Extracting {file_path}...\")\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extracted_path)\n",
    "    else:\n",
    "        print(f\"Using already extracted data at {extracted_path}...\")\n",
    "\n",
    "    # Find the CSV file inside the extracted directory\n",
    "    csv_files = [os.path.join(extracted_path, f) for f in os.listdir(extracted_path) if f.endswith('.tsv')]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {extracted_path}\")\n",
    "\n",
    "    # Load CSV into Spark\n",
    "    print(f\"Loading data from {csv_files[0]}...\")\n",
    "    df = spark.read.csv(csv_files[0], schema=schema, sep=\"\\t\", header=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using already extracted data at /home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train...\n",
      "Loading data from /home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train/behaviors.tsv...\n",
      "Using already extracted data at /home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_dev...\n",
      "Loading data from /home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_dev/behaviors.tsv...\n"
     ]
    }
   ],
   "source": [
    "# Load train and validation data\n",
    "train_raw_df = extract_and_load_zip(train_path, schema)\n",
    "valid_raw_df = extract_and_load_zip(validation_path, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 19:40:12,479 - DataUtils - INFO - Starting to preprocess MIND dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- impression_id: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- history: string (nullable = true)\n",
      " |-- impressions: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- impression_id: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- history: string (nullable = true)\n",
      " |-- impressions: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-26 19:40:12,769 - DataUtils - INFO - Preprocessing of MIND dataset completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- newsId: integer (nullable = true)\n",
      " |-- clicked: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/26 19:40:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: U13740, N55689-1 N35729-0\n",
      " Schema: user_id, impressions\n",
      "Expected: user_id but found: U13740\n",
      "CSV file: file:///home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train/behaviors.tsv\n",
      "24/12/26 19:40:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: U13740, N55689-1 N35729-0\n",
      " Schema: user_id, impressions\n",
      "Expected: user_id but found: U13740\n",
      "CSV file: file:///home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train/behaviors.tsv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|userId|newsId|clicked|\n",
      "+------+------+-------+\n",
      "| 91836| 17059|      1|\n",
      "| 73700| 23814|      1|\n",
      "| 34670| 49685|      1|\n",
      "|  8125|  8400|      1|\n",
      "| 19739| 21119|      1|\n",
      "+------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- newsId: integer (nullable = true)\n",
      " |-- clicked: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/26 19:40:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: U80234, N28682-0 N48740-0 N31958-1 N34130-0 N6916-0 N5472-0 N50775-0 N24802-0 N19990-0 N33176-0 N62365-0 N5940-0 N6400-0 N58098-0 N42844-0 N49285-0 N51470-0 N53572-0 N11930-0 N21679-0 N55237-0 N29862-0\n",
      " Schema: user_id, impressions\n",
      "Expected: user_id but found: U80234\n",
      "CSV file: file:///home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_dev/behaviors.tsv\n",
      "[Stage 103:=========================>                              (5 + 6) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|userId|newsId|clicked|\n",
      "+------+------+-------+\n",
      "| 60458| 23513|      1|\n",
      "| 44190|  5940|      1|\n",
      "| 87380| 15347|      1|\n",
      "|  9444|  5940|      1|\n",
      "|  9444| 31958|      1|\n",
      "+------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/26 19:40:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: U80234, N28682-0 N48740-0 N31958-1 N34130-0 N6916-0 N5472-0 N50775-0 N24802-0 N19990-0 N33176-0 N62365-0 N5940-0 N6400-0 N58098-0 N42844-0 N49285-0 N51470-0 N53572-0 N11930-0 N21679-0 N55237-0 N29862-0\n",
      " Schema: user_id, impressions\n",
      "Expected: user_id but found: U80234\n",
      "CSV file: file:///home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_dev/behaviors.tsv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the raw train and validation datasets\n",
    "#train_raw_df = spark.read.csv(train_csv_path, header=True, schema=schema)\n",
    "#valid_raw_df = spark.read.csv(valid_csv_path, header=True, schema=schema)\n",
    "\n",
    "# Preprocess the datasets\n",
    "npratio = 4  # Define your negative sampling ratio\n",
    "training_data, validation_data = preprocess_behaviors_mind(spark, train_raw_df, valid_raw_df, npratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning setup\n",
    "als = ALS(userCol=\"userId\",\n",
    "          itemCol=\"newsId\",\n",
    "          ratingCol=\"clicked\",\n",
    "          coldStartStrategy=\"drop\",\n",
    "          maxIter=15)\n",
    "\n",
    "# Define the parameter grid without maxIter\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 30, 40]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.05, 0.1, 0.2]) \\\n",
    "    .addGrid(als.alpha, [1.0, 5.0, 10.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"clicked\", predictionCol=\"prediction\"),\n",
    "    numFolds=3,\n",
    "    parallelism=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/26 19:43:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: U13740, N55689-1 N35729-0\n",
      " Schema: user_id, impressions\n",
      "Expected: user_id but found: U13740\n",
      "CSV file: file:///home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train/behaviors.tsv\n",
      "24/12/26 19:43:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: U13740, N55689-1 N35729-0\n",
      " Schema: user_id, impressions\n",
      "Expected: user_id but found: U13740\n",
      "CSV file: file:///home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train/behaviors.tsv\n",
      "24/12/26 19:44:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/12/26 19:44:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/12/26 19:44:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/12/26 19:44:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: U13740, N55689-1 N35729-0\n",
      " Schema: user_id, impressions\n",
      "Expected: user_id but found: U13740\n",
      "CSV file: file:///home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train/behaviors.tsv\n",
      "24/12/26 19:44:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: U13740, N55689-1 N35729-0\n",
      " Schema: user_id, impressions\n",
      "Expected: user_id but found: U13740\n",
      "CSV file: file:///home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train/behaviors.tsv\n",
      "[Stage 3026:===>           (2 + 8) / 10][Stage 3087:=>             (1 + 0) / 10]]]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit cross-validation model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cv_model \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract the best model and parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m cv_model\u001b[38;5;241m.\u001b[39mbestModel\n",
      "File \u001b[0;32m~/.virtualenvs/data_mining/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/data_mining/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit cross-validation model\n",
    "cv_model = cv.fit(training_data)\n",
    "\n",
    "# Extract the best model and parameters\n",
    "best_model = cv_model.bestModel\n",
    "best_rank = best_model._java_obj.parent().getRank()\n",
    "best_reg_param = best_model._java_obj.parent().getRegParam()\n",
    "best_alpha = best_model._java_obj.parent().getAlpha()\n",
    "\n",
    "print(f\"Best Hyperparameters:\\nRank: {best_rank}, RegParam: {best_reg_param}, Alpha: {best_alpha}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
