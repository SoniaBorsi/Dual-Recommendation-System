{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A notebook for Joaco to test stuff related to MIND preprocessing. Will be erased for the final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from recommenders.datasets.mind import (download_mind,\n",
    "                                     extract_mind,\n",
    "                                     download_and_extract_glove,\n",
    "                                     load_glove_matrix,\n",
    "                                     word_tokenize\n",
    "                                    )\n",
    "from recommenders.datasets.download_utils import unzip_file\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIND sizes: \"demo\", \"small\" or \"large\"\n",
    "mind_type=\"demo\" \n",
    "# word_embedding_dim should be in [50, 100, 200, 300]\n",
    "word_embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17.0k/17.0k [00:27<00:00, 628KB/s]  \n",
      "100%|██████████| 9.84k/9.84k [00:14<00:00, 660KB/s]  \n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "train_zip, valid_zip = download_mind(size=mind_type, dest_path=data_path)\n",
    "unzip_file(train_zip, os.path.join(data_path, 'train'), clean_zip_file=False)\n",
    "unzip_file(valid_zip, os.path.join(data_path, 'valid'), clean_zip_file=False)\n",
    "output_path = os.path.join(data_path, 'utils')\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, explode, split, when, lit, rand\n",
    "from pyspark.sql.window import Window\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "#Load training and validation data based on the selected data source.\n",
    "def load_training_data(spark,\n",
    "                       data_source = \"recommenders\",  # \"db\", \"recommenders\", or \"csv\"\n",
    "                       **kwargs):\n",
    "    if data_source == \"recommenders\":\n",
    "        \n",
    "        train_path = \"./data/mind/train/behaviors.tsv\"\n",
    "        valid_path = \"./data/mind/valid/behaviors.tsv\"\n",
    "        \n",
    "        logger.info(\"Preprocessing MIND dataset...\")        \n",
    "        \n",
    "        training_data, validation_data = preprocess_behaviors_mind(\n",
    "            spark=spark,\n",
    "            train_path=train_path,\n",
    "            valid_path=valid_path\n",
    "        )\n",
    "        logger.info(\"MIND dataset preprocessed successfully.\")\n",
    "        \n",
    "    elif data_source == \"db\":\n",
    "        from data_management.data_utils import load_data_split\n",
    "        config = kwargs.get(\"config\")\n",
    "        query = kwargs.get(\"query\")\n",
    "        training_data, validation_data = load_data_split(spark, config=config, query=query)\n",
    "    \n",
    "    elif data_source == \"csv\":\n",
    "        file_path = kwargs.get(\"file_path\", \"./data/csv\")\n",
    "        logger.info(f\"Loading training data from CSV: {file_path}/training_data.csv\")\n",
    "        training_data = spark.read.csv(f\"{file_path}/training_data.csv\", header=True)\n",
    "        logger.info(f\"Loading validation data from CSV: {file_path}/validation_data.csv\")\n",
    "        validation_data = spark.read.csv(f\"{file_path}/validation_data.csv\", header=True)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data source: {data_source}\")\n",
    "\n",
    "    return training_data, validation_data\n",
    "\n",
    "def get_logger(name: str, log_file: str = None, level: int = logging.INFO, max_bytes: int = 10 * 1024 * 1024, backup_count: int = 5):\n",
    "    \n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        \n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(level)\n",
    "\n",
    "        formatter = logging.Formatter(\n",
    "            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "        )\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "        if log_file:\n",
    "            os.makedirs(os.path.dirname(log_file), exist_ok=True) #Create directory if it doesn't exist\n",
    "            \n",
    "            file_handler = RotatingFileHandler(\n",
    "                log_file, maxBytes=max_bytes, backupCount=backup_count\n",
    "            )\n",
    "            file_handler.setLevel(level)\n",
    "            file_handler.setFormatter(formatter)\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = get_logger(\"DataUtils\", log_file=\"logs/data_utils.log\")\n",
    "\n",
    "# Preprocesses the behaviors table for training Spark's ALS model\n",
    "def preprocess_behaviors_mind(\n",
    "    spark: SparkSession, \n",
    "    train_path: str, \n",
    "    valid_path: str, \n",
    "    npratio: int = 4\n",
    "):\n",
    "    logger.info(f\"Starting to preprocess MIND dataset. Train: {train_path}, Valid: {valid_path}\")\n",
    "    \n",
    "    def process_behaviors(df):\n",
    "        impressions_df = df.withColumn(\"impression\", explode(split(col(\"impressions\"), \" \")))\n",
    "        \n",
    "        # Extract clicked (1) or non-clicked (0) status\n",
    "        impressions_df = impressions_df.withColumn(\n",
    "            \"clicked\",\n",
    "            when(col(\"impression\").endswith(\"-1\"), lit(1)).otherwise(lit(0))\n",
    "        ).withColumn(\n",
    "            \"newsId\",\n",
    "            split(col(\"impression\"), \"-\")[0]\n",
    "        ).select(\"userId\", \"newsId\", \"clicked\")\n",
    "        \n",
    "        positive_samples = impressions_df.filter(col(\"clicked\") == 1)\n",
    "        negative_samples = impressions_df.filter(col(\"clicked\") == 0) \\\n",
    "            .withColumn(\"rand\", rand())\n",
    "        \n",
    "        # Select npratio negative samples per positive sample (addressing class imbalance and making the matrix lighter)\n",
    "        window = Window.partitionBy(\"userId\").orderBy(\"rand\")\n",
    "        negative_samples = negative_samples.withColumn(\"rank\", F.row_number().over(window)) \\\n",
    "            .filter(col(\"rank\") <= npratio) \\\n",
    "            .drop(\"rank\", \"rand\")\n",
    "        \n",
    "        combined_samples = positive_samples.union(negative_samples)\n",
    "        \n",
    "        return combined_samples\n",
    "\n",
    "    train_behaviors = spark.read.csv(train_path, sep=\"\\t\", header=False) \\\n",
    "        .toDF(\"impressionId\", \"userId\", \"timestamp\", \"click_history\", \"impressions\")\n",
    "    valid_behaviors = spark.read.csv(valid_path, sep=\"\\t\", header=False) \\\n",
    "        .toDF(\"impressionId\", \"userId\", \"timestamp\", \"click_history\", \"impressions\")\n",
    "    \n",
    "    train_df = process_behaviors(train_behaviors)\n",
    "    valid_df = process_behaviors(valid_behaviors)\n",
    "\n",
    "    logger.info(\"Preprocessing of MIND dataset completed.\")\n",
    "    return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
