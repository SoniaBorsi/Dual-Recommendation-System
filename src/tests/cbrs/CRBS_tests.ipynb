{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, udf, regexp_replace, lit, from_unixtime\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType, StringType, MapType\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, MapType, StringType\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, length, size, udf\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import split, explode, regexp_extract, col, collect_list, udf, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType, FloatType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEWS PRE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:07:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MIND Dataset Processing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|                  []|\n",
      "|N19639|   health|     weightloss|50 Worst Habits F...|These seemingly h...|https://assets.ms...|[{\"Label\": \"Adipo...|[{\"Label\": \"Adipo...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the news.tsv file\n",
    "news_path = \"data/mind/MINDsmall_train/news.tsv\"\n",
    "\n",
    "# Define column names for the news.tsv file\n",
    "news_columns = [\"NewsID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\"]\n",
    "\n",
    "# Load the news.tsv file into a Spark DataFrame\n",
    "news_df = spark.read.csv(\n",
    "    news_path,\n",
    "    sep=\"\\t\",\n",
    "    schema=\"NewsID STRING, Category STRING, Subcategory STRING, Title STRING, Abstract STRING, URL STRING, TitleEntities STRING, AbstractEntities STRING\",\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Assign column names\n",
    "news_df.show(n=2, truncate=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:07:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|NewsID| Category|    Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|N55528|lifestyle|lifestyleroyals|The Brands Queen ...|Shop the notebook...|https://assets.ms...|[{\"Label\": \"Princ...|                  []|\n",
      "|N19639|   health|     weightloss|50 Worst Habits F...|These seemingly h...|https://assets.ms...|[{\"Label\": \"Adipo...|[{\"Label\": \"Adipo...|\n",
      "|N61837|     news|      newsworld|The Cost of Trump...|Lt. Ivan Molchane...|https://assets.ms...|                  []|[{\"Label\": \"Ukrai...|\n",
      "|N53526|   health|         voices|I Was An NBA Wife...|I felt like I was...|https://assets.ms...|                  []|[{\"Label\": \"Natio...|\n",
      "|N38324|   health|        medical|How to Get Rid of...|They seem harmles...|https://assets.ms...|[{\"Label\": \"Skin ...|[{\"Label\": \"Skin ...|\n",
      "+------+---------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PreprocessingPipeline\").getOrCreate()\n",
    "\n",
    "# Load your data (modify the path as necessary)\n",
    "news_df = spark.read.csv(\"data/mind/MINDsmall_train/news.tsv\", sep=\"\\t\", header=False, inferSchema=True)\n",
    "\n",
    "# Assign column names\n",
    "news_df = news_df.toDF(\"NewsID\", \"Category\", \"Subcategory\", \"Title\", \"Abstract\", \"URL\", \"TitleEntities\", \"AbstractEntities\")\n",
    "\n",
    "# Display initial rows\n",
    "news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before dropping missing values: 51282\n",
      "Rows after dropping missing values: 48616\n"
     ]
    }
   ],
   "source": [
    "### MISSING VALUES ###\n",
    "print(f\"Rows before dropping missing values: {news_df.count()}\")\n",
    "\n",
    "# Drop rows where Title or Abstract are missing\n",
    "news_df = news_df.na.drop(subset=[\"Title\", \"Abstract\"])\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Rows after dropping missing values: {news_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CleanTitle                                                            |CleanAbstract                                                                                                                                                                                       |\n",
      "+----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|the brands queen elizabeth, prince charles, and prince philip swear by|shop the notebooks, jackets, and more that the royals can't live without.                                                                                                                           |\n",
      "|50 worst habits for belly fat                                         |these seemingly harmless habits are holding you back and keeping you from shedding that unwanted belly fat for good.                                                                                |\n",
      "|the cost of trump's aid freeze in the trenches of ukraine's war       |lt. ivan molchanets peeked over a parapet of sand bags at the front line of the war in ukraine. next to him was an empty helmet propped up to trick snipers, already perforated with multiple holes.|\n",
      "|i was an nba wife. here's how it affected my mental health.           |i felt like i was a fraud, and being an nba wife didn't help that. in fact, it nearly destroyed me.                                                                                                 |\n",
      "|how to get rid of skin tags, according to a dermatologist             |they seem harmless, but there's a very good reason you shouldn't ignore them. the post how to get rid of skin tags, according to a dermatologist appeared first on reader's digest.                 |\n",
      "+----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TEXT CLEANING ###\n",
    "\n",
    "# Define a function to clean text (remove special characters and convert to lowercase)\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        return text.lower().replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    return None\n",
    "\n",
    "# Register the UDF\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "\n",
    "# Apply text cleaning to Title and Abstract\n",
    "news_df = news_df.withColumn(\"CleanTitle\", clean_text_udf(col(\"Title\")))\n",
    "news_df = news_df.withColumn(\"CleanAbstract\", clean_text_udf(col(\"Abstract\")))\n",
    "\n",
    "# Display cleaned text\n",
    "news_df.select(\"CleanTitle\", \"CleanAbstract\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|TitleTokens                                                                       |AbstractTokens                                                                                                                                                                                                                           |\n",
      "+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[the, brands, queen, elizabeth,, prince, charles,, and, prince, philip, swear, by]|[shop, the, notebooks,, jackets,, and, more, that, the, royals, can't, live, without.]                                                                                                                                                   |\n",
      "|[50, worst, habits, for, belly, fat]                                              |[these, seemingly, harmless, habits, are, holding, you, back, and, keeping, you, from, shedding, that, unwanted, belly, fat, for, good.]                                                                                                 |\n",
      "|[the, cost, of, trump's, aid, freeze, in, the, trenches, of, ukraine's, war]      |[lt., ivan, molchanets, peeked, over, a, parapet, of, sand, bags, at, the, front, line, of, the, war, in, ukraine., next, to, him, was, an, empty, helmet, propped, up, to, trick, snipers,, already, perforated, with, multiple, holes.]|\n",
      "|[i, was, an, nba, wife., here's, how, it, affected, my, mental, health.]          |[i, felt, like, i, was, a, fraud,, and, being, an, nba, wife, didn't, help, that., in, fact,, it, nearly, destroyed, me.]                                                                                                                |\n",
      "|[how, to, get, rid, of, skin, tags,, according, to, a, dermatologist]             |[they, seem, harmless,, but, there's, a, very, good, reason, you, shouldn't, ignore, them., the, post, how, to, get, rid, of, skin, tags,, according, to, a, dermatologist, appeared, first, on, reader's, digest.]                      |\n",
      "+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TOKENIZATION ###\n",
    "\n",
    "# Tokenize CleanTitle and CleanAbstract\n",
    "tokenizer_title = Tokenizer(inputCol=\"CleanTitle\", outputCol=\"TitleTokens\")\n",
    "tokenizer_abstract = Tokenizer(inputCol=\"CleanAbstract\", outputCol=\"AbstractTokens\")\n",
    "\n",
    "news_df = tokenizer_title.transform(news_df)\n",
    "news_df = tokenizer_abstract.transform(news_df)\n",
    "\n",
    "# Display tokenized data\n",
    "news_df.select(\"TitleTokens\", \"AbstractTokens\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|FilteredTitleTokens                                                 |FilteredAbstractTokens                                                                                                                                                |\n",
      "+--------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[brands, queen, elizabeth,, prince, charles,, prince, philip, swear]|[shop, notebooks,, jackets,, royals, live, without.]                                                                                                                  |\n",
      "|[50, worst, habits, belly, fat]                                     |[seemingly, harmless, habits, holding, back, keeping, shedding, unwanted, belly, fat, good.]                                                                          |\n",
      "|[cost, trump's, aid, freeze, trenches, ukraine's, war]              |[lt., ivan, molchanets, peeked, parapet, sand, bags, front, line, war, ukraine., next, empty, helmet, propped, trick, snipers,, already, perforated, multiple, holes.]|\n",
      "|[nba, wife., affected, mental, health.]                             |[felt, like, fraud,, nba, wife, help, that., fact,, nearly, destroyed, me.]                                                                                           |\n",
      "|[get, rid, skin, tags,, according, dermatologist]                   |[seem, harmless,, good, reason, ignore, them., post, get, rid, skin, tags,, according, dermatologist, appeared, first, reader's, digest.]                             |\n",
      "+--------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### STOPWORDS REMOVAL ###\n",
    "\n",
    "# Remove stopwords from TitleTokens and AbstractTokens\n",
    "stopword_remover_title = StopWordsRemover(inputCol=\"TitleTokens\", outputCol=\"FilteredTitleTokens\")\n",
    "stopword_remover_abstract = StopWordsRemover(inputCol=\"AbstractTokens\", outputCol=\"FilteredAbstractTokens\")\n",
    "\n",
    "news_df = stopword_remover_title.transform(news_df)\n",
    "news_df = stopword_remover_abstract.transform(news_df)\n",
    "\n",
    "# Display filtered tokens\n",
    "news_df.select(\"FilteredTitleTokens\", \"FilteredAbstractTokens\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to clean each token in the array\n",
    "def clean_tokens(tokens):\n",
    "    if tokens:\n",
    "        return [token.replace(\",\", \"\") for token in tokens]  # Remove commas\n",
    "    return tokens\n",
    "\n",
    "# Register the UDF\n",
    "clean_tokens_udf = udf(clean_tokens, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to FilteredTitleTokens\n",
    "news_df = news_df.withColumn(\"FilteredTitleTokens\", clean_tokens_udf(col(\"FilteredTitleTokens\")))\n",
    "news_df = news_df.withColumn(\"FilteredAbstractTokens\", clean_tokens_udf(col(\"FilteredAbstractTokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+------------------------------------------------------------------+\n",
      "|Title                                                                 |FilteredTitleTokens                                               |\n",
      "+----------------------------------------------------------------------+------------------------------------------------------------------+\n",
      "|The Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By|[brands, queen, elizabeth, prince, charles, prince, philip, swear]|\n",
      "|50 Worst Habits For Belly Fat                                         |[50, worst, habits, belly, fat]                                   |\n",
      "|The Cost of Trump's Aid Freeze in the Trenches of Ukraine's War       |[cost, trump's, aid, freeze, trenches, ukraine's, war]            |\n",
      "|I Was An NBA Wife. Here's How It Affected My Mental Health.           |[nba, wife., affected, mental, health.]                           |\n",
      "|How to Get Rid of Skin Tags, According to a Dermatologist             |[get, rid, skin, tags, according, dermatologist]                  |\n",
      "+----------------------------------------------------------------------+------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"Title\", \"FilteredTitleTokens\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
      "|Abstract                                                                                                            |FilteredAbstractTokens                                                                      |\n",
      "+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
      "|Shop the notebooks, jackets, and more that the royals can't live without.                                           |[shop, notebooks, jackets, royals, live, without.]                                          |\n",
      "|These seemingly harmless habits are holding you back and keeping you from shedding that unwanted belly fat for good.|[seemingly, harmless, habits, holding, back, keeping, shedding, unwanted, belly, fat, good.]|\n",
      "+--------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"Abstract\", \"FilteredAbstractTokens\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NewsID',\n",
       " 'Category',\n",
       " 'Subcategory',\n",
       " 'Title',\n",
       " 'Abstract',\n",
       " 'URL',\n",
       " 'TitleEntities',\n",
       " 'AbstractEntities',\n",
       " 'CleanTitle',\n",
       " 'CleanAbstract',\n",
       " 'TitleTokens',\n",
       " 'AbstractTokens',\n",
       " 'FilteredTitleTokens',\n",
       " 'FilteredAbstractTokens']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+----------------------+\n",
      "|NewsID| Category| FilteredTitleTokens|FilteredAbstractTokens|\n",
      "+------+---------+--------------------+----------------------+\n",
      "|N55528|lifestyle|[brands, queen, e...|  [shop, notebooks,...|\n",
      "+------+---------+--------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select('NewsID', 'Category', 'FilteredTitleTokens', 'FilteredAbstractTokens').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp==5.5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (5.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spark-nlp==5.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.base import DocumentAssembler, TokenAssembler\n",
    "from sparknlp.annotator import BertEmbeddings\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.sql.functions import col, concat_ws, array_union, explode\n",
    "from pyspark.sql.functions import concat, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NewsID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Subcategory: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Abstract: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- TitleEntities: string (nullable = true)\n",
      " |-- AbstractEntities: string (nullable = true)\n",
      " |-- CleanTitle: string (nullable = true)\n",
      " |-- CleanAbstract: string (nullable = true)\n",
      " |-- TitleTokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- AbstractTokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- FilteredTitleTokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- FilteredAbstractTokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.withColumn(\n",
    "    \"combined_tokens\", concat(col(\"FilteredTitleTokens\"), col(\"FilteredAbstractTokens\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+--------------------------------------------------+\n",
      "|FilteredTitleTokens                                               |FilteredAbstractTokens                            |\n",
      "+------------------------------------------------------------------+--------------------------------------------------+\n",
      "|[brands, queen, elizabeth, prince, charles, prince, philip, swear]|[shop, notebooks, jackets, royals, live, without.]|\n",
      "+------------------------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select(\"FilteredTitleTokens\", \"FilteredAbstractTokens\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "|combined_tokens                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "|[brands, queen, elizabeth, prince, charles, prince, philip, swear, shop, notebooks, jackets, royals, live, without.]|\n",
      "+--------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df.select('combined_tokens').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Term frequency \n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"combined_tokens\", outputCol=\"raw_features\")\n",
    "cv_model = cv.fit(news_df)\n",
    "news_df_tf = cv_model.transform(news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:07:33 WARN DAGScheduler: Broadcasting large task binary with size 1236.3 KiB\n",
      "24/12/12 18:07:37 WARN DAGScheduler: Broadcasting large task binary with size 1237.3 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tf_idf\")\n",
    "idf_model = idf.fit(news_df_tf)\n",
    "news_df_tfidf = idf_model.transform(news_df_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:07:38 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|NewsID|     combined_tokens|              tf_idf|\n",
      "+------+--------------------+--------------------+\n",
      "|N55528|[brands, queen, e...|(109675,[310,977,...|\n",
      "|N19639|[50, worst, habit...|(109675,[27,437,8...|\n",
      "|N61837|[cost, trump's, a...|(109675,[63,176,3...|\n",
      "|N53526|[nba, wife., affe...|(109675,[38,89,23...|\n",
      "|N38324|[get, rid, skin, ...|(109675,[6,17,20,...|\n",
      "| N2073|[nfl, able, fine,...|(109675,[87,94,17...|\n",
      "|N49186|[orlando's, hotte...|(109675,[98,224,2...|\n",
      "|N59295|[chile:, three, d...|(109675,[2,19,23,...|\n",
      "|N24510|[best, ps5, games...|(109675,[13,28,33...|\n",
      "|N39237|[report, weather-...|(109675,[52,65,93...|\n",
      "| N9721|[50, foods, never...|(109675,[17,187,2...|\n",
      "|N60905|[trying, make, ra...|(109675,[1,44,58,...|\n",
      "|N39758|[25, biggest, gro...|(109675,[5,207,22...|\n",
      "|N28361|[instagram, filte...|(109675,[142,181,...|\n",
      "|N18680|[michigan, apple,...|(109675,[156,181,...|\n",
      "|N55610|[kate, middleton'...|(109675,[33,35,41...|\n",
      "|N35621|[stars, got, fire...|(109675,[27,72,87...|\n",
      "|N22850|[newark, liberty,...|(109675,[5,8,714,...|\n",
      "|N58173|[2021, gmc, yukon...|(109675,[100,558,...|\n",
      "|N29120|[john, dorsey, ad...|(109675,[2,7,14,1...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_df_tfidf.select(\"NewsID\", \"combined_tokens\", \"tf_idf\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ImpressionID|UserID|Time                 |History                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |Impressions                                                                                                                                                                                                                                                                                                                     |\n",
      "+------------+------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1           |U13740|11/11/2019 9:05:58 AM|N55189 N42782 N34694 N45794 N18445 N63302 N10414 N19347 N31801                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |N55689-1 N35729-0                                                                                                                                                                                                                                                                                                               |\n",
      "|2           |U91836|11/12/2019 6:11:30 PM|N31739 N6072 N63045 N23979 N35656 N43353 N8129 N1569 N17686 N13008 N21623 N6233 N14340 N48031 N62285 N44383 N23061 N16290 N6244 N45099 N58715 N59049 N7023 N50528 N42704 N46082 N8275 N15710 N59026 N8429 N30867 N56514 N19709 N31402 N31741 N54889 N9798 N62612 N2663 N16617 N6087 N13231 N63317 N61388 N59359 N51163 N30698 N34567 N54225 N32852 N55833 N64467 N3142 N13912 N29802 N44462 N29948 N4486 N5398 N14761 N47020 N65112 N31699 N37159 N61101 N14761 N3433 N10438 N61355 N21164 N22976 N2511 N48390 N58224 N48742 N35458 N24611 N37509 N21773 N41011 N19041 N25785|N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N22407-0 N14592-0 N17059-1 N33677-0 N7821-0 N6890-0                                                                                                                                                                                                                                |\n",
      "|3           |U73700|11/14/2019 7:01:48 AM|N10732 N25792 N7563 N21087 N41087 N5445 N60384 N46616 N52500 N33164 N47289 N24233 N62058 N26378 N49475 N18870                                                                                                                                                                                                                                                                                                                                                                                                                                                                |N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N59685-0 N23814-1 N23446-0 N64174-0 N11817-0 N60550-0 N48225-0 N45509-0 N56711-0 N46821-0 N48017-0 N8015-0 N5364-0 N48722-0 N55555-0 N37348-0 N40109-0 N59495-0 N36226-0 N38779-0 N47346-0 N48875-0 N10960-0 N29739-0 N50872-0 N50592-0 N13131-0 N3839-0 N12330-0 N47098-0 N51570-0|\n",
      "+------------+------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "behaviors_schema = StructType([\n",
    "    StructField(\"ImpressionID\", StringType(), True),\n",
    "    StructField(\"UserID\", StringType(), True),\n",
    "    StructField(\"Time\", StringType(), True),\n",
    "    StructField(\"History\", StringType(), True),\n",
    "    StructField(\"Impressions\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the behaviors.tsv file\n",
    "behaviors_df = spark.read.csv(\n",
    "    \"data/mind/MINDsmall_train/behaviors.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    schema=behaviors_schema,\n",
    "    header=False\n",
    ")\n",
    "\n",
    "# Display the schema and a sample row\n",
    "# behaviors_df.printSchema()\n",
    "behaviors_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ImpressionID|UserID|HistoryList                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+------------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1           |U13740|[N55189, N42782, N34694, N45794, N18445, N63302, N10414, N19347, N31801]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|2           |U91836|[N31739, N6072, N63045, N23979, N35656, N43353, N8129, N1569, N17686, N13008, N21623, N6233, N14340, N48031, N62285, N44383, N23061, N16290, N6244, N45099, N58715, N59049, N7023, N50528, N42704, N46082, N8275, N15710, N59026, N8429, N30867, N56514, N19709, N31402, N31741, N54889, N9798, N62612, N2663, N16617, N6087, N13231, N63317, N61388, N59359, N51163, N30698, N34567, N54225, N32852, N55833, N64467, N3142, N13912, N29802, N44462, N29948, N4486, N5398, N14761, N47020, N65112, N31699, N37159, N61101, N14761, N3433, N10438, N61355, N21164, N22976, N2511, N48390, N58224, N48742, N35458, N24611, N37509, N21773, N41011, N19041, N25785]|\n",
      "|3           |U73700|[N10732, N25792, N7563, N21087, N41087, N5445, N60384, N46616, N52500, N33164, N47289, N24233, N62058, N26378, N49475, N18870]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+------------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split History into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"HistoryList\", split(col(\"History\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"History\")  # Drop original History column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"UserID\", \"HistoryList\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------------------------------------------------------------------------------+\n",
      "|ImpressionID|ImpressionsList                                                                                             |\n",
      "+------------+------------------------------------------------------------------------------------------------------------+\n",
      "|1           |[N55689-1, N35729-0]                                                                                        |\n",
      "|2           |[N20678-0, N39317-0, N58114-0, N20495-0, N42977-0, N22407-0, N14592-0, N17059-1, N33677-0, N7821-0, N6890-0]|\n",
      "+------------+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split Impressions into an array\n",
    "behaviors_df = behaviors_df.withColumn(\"ImpressionsList\", split(col(\"Impressions\"), \" \"))\n",
    "behaviors_df = behaviors_df.drop(\"Impressions\")  # Drop original Impressions column if not needed\n",
    "\n",
    "# Verify the transformation\n",
    "behaviors_df.select(\"ImpressionID\", \"ImpressionsList\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---------------+----------+\n",
      "|ImpressionID|UserID|CandidateNewsID|ClickLabel|\n",
      "+------------+------+---------------+----------+\n",
      "|1           |U13740|N55689         |1         |\n",
      "|1           |U13740|N35729         |0         |\n",
      "|2           |U91836|N20678         |0         |\n",
      "|2           |U91836|N39317         |0         |\n",
      "|2           |U91836|N58114         |0         |\n",
      "+------------+------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode ImpressionsList\n",
    "impressions_exploded = behaviors_df.select(\n",
    "    \"ImpressionID\",\n",
    "    \"UserID\",\n",
    "    \"Time\",\n",
    "    \"HistoryList\",\n",
    "    explode(\"ImpressionsList\").alias(\"ImpressionItem\")\n",
    ")\n",
    "\n",
    "# Extract CandidateNewsID and ClickLabel using regex\n",
    "impressions_exploded = impressions_exploded \\\n",
    "    .withColumn(\"CandidateNewsID\", regexp_extract(col(\"ImpressionItem\"), r\"^(N\\d+)-\\d+$\", 1)) \\\n",
    "    .withColumn(\"ClickLabel\", regexp_extract(col(\"ImpressionItem\"), r\"^N\\d+-(\\d+)$\", 1).cast(\"integer\")) \\\n",
    "    .drop(\"ImpressionItem\")\n",
    "\n",
    "# Verify the transformation\n",
    "impressions_exploded.select(\"ImpressionID\", \"UserID\", \"CandidateNewsID\", \"ClickLabel\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join impressions with news_features_df on CandidateNewsID\n",
    "impressions_with_features = impressions_exploded.join(\n",
    "    news_df_tfidf,\n",
    "    impressions_exploded.CandidateNewsID == news_df_tfidf.NewsID,\n",
    "    how=\"left\"\n",
    ").drop(news_df_tfidf.NewsID)  # Drop duplicate NewsID column if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:09:47 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/12/12 18:09:55 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "|ImpressionID|UserID|                Time|         HistoryList|CandidateNewsID|ClickLabel|    Category|       Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|          CleanTitle|       CleanAbstract|         TitleTokens|      AbstractTokens| FilteredTitleTokens|FilteredAbstractTokens|     combined_tokens|        raw_features|              tf_idf|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "|      143656|U74335|11/11/2019 3:07:0...|[N13008, N19593, ...|         N11483|         0|       autos|         autossuvs|2020 Hyundai Venu...|Hyundai's new tin...|https://assets.ms...|                  []|[{\"Label\": \"Nissa...|2020 hyundai venu...|hyundai's new tin...|[2020, hyundai, v...|[hyundai's, new, ...|[2020, hyundai, v...|  [hyundai's, new, ...|[2020, hyundai, v...|(109675,[1,85,301...|(109675,[1,85,301...|\n",
      "|       81980|U11232|11/10/2019 1:26:1...|[N63248, N43558, ...|         N29341|         0|   lifestyle|lifestyleparenting|100 vintage baby ...|Stacker looks at ...|https://assets.ms...|                  []|[{\"Label\": \"Bible...|100 vintage baby ...|stacker looks at ...|[100, vintage, ba...|[stacker, looks, ...|[100, vintage, ba...|  [stacker, looks, ...|[100, vintage, ba...|(109675,[27,213,2...|(109675,[27,213,2...|\n",
      "|       61593|U71196|11/14/2019 10:15:...|[N25300, N11101, ...|         N59841|         0|foodanddrink|           recipes|32 Things You Can...|Looking for a hea...|https://assets.ms...|[{\"Label\": \"Air f...|[{\"Label\": \"Air f...|32 things you can...|looking for a hea...|[32, things, you,...|[looking, for, a,...|[32, things, make...|  [looking, healthi...|[32, things, make...|(109675,[1,6,44,1...|(109675,[1,6,44,1...|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "impressions_with_features.show(3, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:32:03 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/12/12 18:32:08 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|ImpressionID|UserID|                Time|         HistoryList|CandidateNewsID|ClickLabel|    Category|       Subcategory|               Title|            Abstract|                 URL|       TitleEntities|    AbstractEntities|              tf_idf|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      143656|U74335|11/11/2019 3:07:0...|[N13008, N19593, ...|         N11483|         0|       autos|         autossuvs|2020 Hyundai Venu...|Hyundai's new tin...|https://assets.ms...|                  []|[{\"Label\": \"Nissa...|(109675,[1,85,301...|\n",
      "|       81980|U11232|11/10/2019 1:26:1...|[N63248, N43558, ...|         N26084|         1|foodanddrink|           recipes|These Christmas D...|We've got you cov...|https://assets.ms...|                  []|                  []|(109675,[120,559,...|\n",
      "|      102498|U68128|11/13/2019 12:56:...|[N34930, N50, N46...|         N28523|         0|        news|            newsus|Red tide, the tox...|Southwest Florida...|https://assets.ms...|[{\"Label\": \"South...|[{\"Label\": \"South...|(109675,[22,42,97...|\n",
      "|       81980|U11232|11/10/2019 1:26:1...|[N63248, N43558, ...|         N29341|         0|   lifestyle|lifestyleparenting|100 vintage baby ...|Stacker looks at ...|https://assets.ms...|                  []|[{\"Label\": \"Bible...|(109675,[27,213,2...|\n",
      "|           2|U91836|11/12/2019 6:11:3...|[N31739, N6072, N...|         N58114|         0|        NULL|              NULL|                NULL|                NULL|                NULL|                NULL|                NULL|                NULL|\n",
      "+------------+------+--------------------+--------------------+---------------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "    # Filter records where ClickLabel == 1\n",
    "    clicked_news_df = impressions_with_features.filter(col(\"ClickLabel\") == 1)\n",
    "    clicked_news_df = impressions_with_features.drop('CleanTitle', 'CleanAbstract', 'TitleTokens', 'AbstractTokens', 'AbstractTokens', 'FilteredAbstractTokens','FilteredAbstractTokens', 'FilteredAbstractTokens', 'raw_features','combined_tokens','FilteredTitleTokens' )\n",
    "    # Verify the filtered DataFrame\n",
    "    clicked_news_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:22:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"User Embeddings\") \\\n",
    "    .config(\"spark.pyspark.python\", \"/path/to/python\") \\\n",
    "    .config(\"spark.pyspark.driver.python\", \"/path/to/python\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import collect_list, col, udf\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# import numpy as np\n",
    "\n",
    "# # UDF to compute the average of a list of vectors\n",
    "# def average_vectors(vectors):\n",
    "#     if not vectors:\n",
    "#         return Vectors.dense([0.0] * 109675)  # Replace 109675 with the actual embedding size\n",
    "#     np_vectors = np.array([v.toArray() for v in vectors])\n",
    "#     avg_vector = np.mean(np_vectors, axis=0)\n",
    "#     return Vectors.dense(avg_vector)\n",
    "\n",
    "# average_vectors_udf = udf(average_vectors, VectorUDT())\n",
    "\n",
    "# def test_average_vectors():\n",
    "#     vectors = [Vectors.dense([1.0, 2.0, 3.0]), Vectors.dense([4.0, 5.0, 6.0])]\n",
    "#     np_vectors = np.array([v.toArray() for v in vectors])\n",
    "#     avg_vector = np.mean(np_vectors, axis=0)\n",
    "#     print(avg_vector)\n",
    "\n",
    "# test_average_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:23:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"User Embeddings\") \\\n",
    "    .config(\"spark.pyspark.python\", \"/usr/local/bin/python3\") \\\n",
    "    .config(\"spark.pyspark.driver.python\", \"/usr/local/bin/python3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ImpressionID: string (nullable = true)\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- HistoryList: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- CandidateNewsID: string (nullable = false)\n",
      " |-- ClickLabel: integer (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Subcategory: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Abstract: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- TitleEntities: string (nullable = true)\n",
      " |-- AbstractEntities: string (nullable = true)\n",
      " |-- tf_idf: vector (nullable = true)\n",
      " |-- tf_idf_array: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clicked_news_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_news_df = clicked_news_df.filter(col(\"tf_idf\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ImpressionID: string (nullable = true)\n",
      " |-- UserID: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- HistoryList: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- CandidateNewsID: string (nullable = false)\n",
      " |-- ClickLabel: integer (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Subcategory: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Abstract: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- TitleEntities: string (nullable = true)\n",
      " |-- AbstractEntities: string (nullable = true)\n",
      " |-- tf_idf: vector (nullable = true)\n",
      " |-- tf_idf_array: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 18:29:38 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              tf_idf|\n",
      "+--------------------+\n",
      "|(109675,[8,30,51,...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clicked_news_df.printSchema()\n",
    "clicked_news_df.select('tf_idf').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@pandas_udf(\"array<float>\")\n",
    "def average_vectors_udf(vectors: pd.Series) -> pd.Series:\n",
    "    # Convert list of vectors to a NumPy array and calculate the mean\n",
    "    np_vectors = np.array(vectors.tolist())\n",
    "    avg_vector = np.mean(np_vectors, axis=0)\n",
    "    return pd.Series(avg_vector.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
