{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "from src.utilities.data_utils import preprocess_behaviors_mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train.zip\"\n",
    "validation_path = \"/home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_dev.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/27 11:49:13 WARN Utils: Your hostname, DESKTOP-LQJ6T08 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/27 11:49:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/27 11:49:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark session initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ALS Hyperparameter Tuning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema for loading the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"impression_id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"history\", StringType(), True),\n",
    "    StructField(\"impressions\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract and preprocess data\n",
    "def extract_and_load_zip(file_path, schema):\n",
    "    # Create a temporary directory for extraction\n",
    "    extracted_path = os.path.splitext(file_path)[0]\n",
    "    if not os.path.exists(extracted_path):\n",
    "        print(f\"Extracting {file_path}...\")\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extracted_path)\n",
    "    else:\n",
    "        print(f\"Using already extracted data at {extracted_path}...\")\n",
    "\n",
    "    # Find the CSV file inside the extracted directory\n",
    "    csv_files = [os.path.join(extracted_path, f) for f in os.listdir(extracted_path) if f.endswith('.tsv')]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {extracted_path}\")\n",
    "\n",
    "    # Load CSV into Spark\n",
    "    print(f\"Loading data from {csv_files[0]}...\")\n",
    "    df = spark.read.csv(csv_files[0], schema=schema, sep=\"\\t\", header=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using already extracted data at /home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train...\n",
      "Loading data from /home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_train/behaviors.tsv...\n",
      "Using already extracted data at /home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_dev...\n",
      "Loading data from /home/joaquin_l_calvo/Trento/Data_Mining/MINDsmall_dev/behaviors.tsv...\n"
     ]
    }
   ],
   "source": [
    "# Load train and validation data\n",
    "train_raw_df = extract_and_load_zip(train_path, schema)\n",
    "valid_raw_df = extract_and_load_zip(validation_path, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 11:49:20,036 - DataUtils - INFO - Starting to preprocess MIND dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- impression_id: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- history: string (nullable = true)\n",
      " |-- impressions: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- impression_id: integer (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- history: string (nullable = true)\n",
      " |-- impressions: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-27 11:49:21,523 - DataUtils - INFO - Preprocessing of MIND dataset completed.\n"
     ]
    }
   ],
   "source": [
    "# Load the raw train and validation datasets\n",
    "#train_raw_df = spark.read.csv(train_csv_path, header=True, schema=schema)\n",
    "#valid_raw_df = spark.read.csv(valid_csv_path, header=True, schema=schema)\n",
    "\n",
    "# Preprocess the datasets\n",
    "npratio = 4  # Define your negative sampling ratio\n",
    "training_data, validation_data = preprocess_behaviors_mind(spark, train_raw_df, valid_raw_df, npratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning setup\n",
    "als = ALS(userCol=\"userId\",\n",
    "          itemCol=\"newsId\",\n",
    "          ratingCol=\"clicked\",\n",
    "          coldStartStrategy=\"drop\",\n",
    "          maxIter=15)\n",
    "\n",
    "# Define the parameter grid without maxIter\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 30, 40]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.05, 0.1, 0.2]) \\\n",
    "    .addGrid(als.alpha, [1.0, 5.0, 10.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"clicked\", predictionCol=\"prediction\"),\n",
    "    numFolds=3,\n",
    "    parallelism=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/27 11:50:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/12/27 11:50:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/12/27 11:50:10 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                ]]]]]6]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "Rank: 40, RegParam: 0.1, Alpha: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Fit cross-validation model\n",
    "cv_model = cv.fit(training_data)\n",
    "\n",
    "# Extract the best model and parameters\n",
    "best_model = cv_model.bestModel\n",
    "best_rank = best_model._java_obj.parent().getRank()\n",
    "best_reg_param = best_model._java_obj.parent().getRegParam()\n",
    "best_alpha = best_model._java_obj.parent().getAlpha()\n",
    "\n",
    "print(f\"Best Hyperparameters:\\nRank: {best_rank}, RegParam: {best_reg_param}, Alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
